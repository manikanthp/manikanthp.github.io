---
layout: post
title: Kaggle- Google quest Q&A labelling
featured-img: ml
summary: The google quest Q&A labelling competition conducted by google to build better subjective question-answering algorithms. This Q&A model will be feed to the downstream of anther model for answering or quesitoning to understand and provide more human like conversation. At present computers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences.
---


<section><div><div class="hn as qx qy qz ns"></div><div class="jy ra rb rc rd"><div class=""><h1 id="821d" class="pw-post-title re lb rf bn hp rg rh ri rj rk rl rm rn ro rp rq rr rs rt ru rv rw rx ry rz sa fl" data-selectable-paragraph="">Google QUEST Q&amp;A Labeling: kaggle competition</h1></div><figure class="pt pu sc sd se sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm sb"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*Z3GT4j_Y-KxNhFJ9OnJJiA.jpeg" loading="lazy" role="presentation" width="700" height="467"></div></div><figcaption class="sm bm ps jl jm sn so bn b bo bp fk" data-selectable-paragraph="">Photo by <a class="au sp" href="https://unsplash.com/@kellysikkema" rel="noopener ugc nofollow" target="_blank">Kelly Sikkema</a> on<a class="au sp" href="https://unsplash.com/photos/v9FQR4tbIq8" rel="noopener ugc nofollow" target="_blank"> unsplash</a></figcaption></figure><h1 id="74a7" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Table of Contents:</h1><ol class=""><li id="fabc" class="td te rf ky b tf tg th ti tj tk tl tm tn to tp tq tr ts tt fl" data-selectable-paragraph="">Introduction</li><li id="9aa0" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph="">Business problem</li><li id="a11a" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph="">About the data set</li><li id="3672" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph="">Prerequisites</li><li id="afe6" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph="">EDA</li><li id="8c28" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph="">Feature engineering</li><li id="f49d" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph="">Data Preprocessing</li><li id="c2cb" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph="">Experiments performed</li><li id="af31" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph="">Comparison of models</li><li id="ac6d" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph="">Final solution</li><li id="fc4a" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph="">Kaggle submission</li><li id="7b49" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph="">Flask demo app</li><li id="5ef6" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph="">Future work</li><li id="e77c" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph="">Reference</li></ol></div><div class="o dz oz tz lw ua" role="separator"><span class="ub hk cj uc ud hi"></span><span class="ub hk cj uc ud hi"></span><span class="ub hk cj uc ud"></span></div><div class="jy ra rb rc rd"><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div class="jl jm ue"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/625/0*2axlTTr6bGZGeNoY.gif" loading="lazy" role="presentation" width="500" height="226"></div></figure><h1 id="1612" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Introduction:</h1><p id="f00e" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">The google quest Q&amp;A labelling competition conducted by google to build better subjective question-answering algorithms. This Q&amp;A model will be feed to the downstream of anther model for answering or quesitoning to understand and provide more human like conversation. At present computers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences.</p><p id="057c" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">We humans are better at addressing subjective questions that require a deeper, multidimensional understanding of context — something computers aren’t trained to do well…yet.. Questions can take many forms — some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem. They can have multiple intents, or seek advice and opinions. Some may be helpful and others interesting. Some are simple right or wrong.</p><p id="8df6" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">So the <a class="au sp" href="https://crowdsource.google.com/" rel="noopener ugc nofollow" target="_blank">CrowdSource</a> team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of these quality scoring aspects.</p><h1 id="bd3c" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Business problem:</h1><p id="d0e4" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">To create a more human-like question and answering system that can answer the provided question having the intuitive understanding of the question. This can attract users and address their question more human-like and this can also increase the number of user participation in the question answering forms and create human-like conversation chat boxes.</p><blockquote class="vb vc vd"><p id="bafc" class="uj uk ve ky b tf uw hr ul th ux hv um vf uy uo up vg uz ur us vh va uu uv tp jy fl" data-selectable-paragraph=""><strong class="ky hp"><em class="rf">Problem statement:</em></strong><em class="rf"> Create intelligent question and answer systems that can reliably predict context without relying on complicated and opaque rating guidelines.</em></p></blockquote><h1 id="864d" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">About the dataset:</h1><p id="4721" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">The data for this competition includes questions and answers from 70 StackExchange similar websites. our task is to predict target values of 30 labels for each question-answer pair. Target labels with the prefix <code class="ho vi vj vk vl b">question_</code> relate to the <code class="ho vi vj vk vl b">question_title</code> and/or <code class="ho vi vj vk vl b">question_body</code> features in the data. Target labels with the prefix <code class="ho vi vj vk vl b">answer_</code> relate to the <code class="ho vi vj vk vl b">answer</code> feature.</p><p id="f7c0" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Each row contains a single question and a single answer to that question, along with additional features (like question user name, answer user name, user url, different categories of questions etc). The training data contains rows with some duplicated questions (but with different answers). The test data does not contain any duplicated questions. This is not a binary prediction challenge. Target labels are aggregated from multiple raters, and can have continuous values in the range <code class="ho vi vj vk vl b">[0,1]</code>. Therefore, predictions must also be in that range.</p><p id="6af8" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">As we have to predict 30 labels with values range from 0 to 1 for instance(one row/ single datapoint) we can say this is a “<strong class="ky hp">multi-label regression problem</strong>”.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm vm"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/0*9JDKq81hVRTbF3Oa.jpeg" loading="lazy" role="presentation" width="700" height="349"></div></div></figure><p id="4925" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">If you have not understood about the data do not worry we will perform EDA on the data set in below section</p><h1 id="acf3" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Prerequisites</h1><p id="bf65" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">This post assumes that you are already familiar with Machine Learning Technique like Regression such as Linear Regression , Linear Support Vector Machine and deep Learning Technique like Multi-layered Perceptrons, Convolution Neural Networks , LSTM, Underfitting, Overfitting, Probability, Text Processing, Python syntax and data structures, Keras library, etc.</p><p id="f4d6" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Natural Language preprocessing transformer models in this post we will be using bert model.</p><h1 id="42be" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Exploratory data analysis:</h1><p id="c18d" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">Let’s take a look at all available features and labels available and its type and get some insights from the data.</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="1587" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">I have used kaggle notebook for this challenge and by running the above cell we get know that we have total <strong class="ky hp">6079 training instances</strong> and in training data set we have 41 columns in which <strong class="ky hp">30 are class labels</strong> and 11 are features. where as in test data set we have 11 feature with no labels and in submission data set we have 31 feature column that one column is for unique ID and 30 are class labels.</p><p id="f01e" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Lets have a look at data using data frame head method</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm vn"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*mWDNajnjgVhg6wZJ8SiLfA.png" loading="lazy" role="presentation" width="700" height="191"></div></div></figure><p id="c791" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Lets run data frame info method to check if there are any null values and type of data available in our train data set</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="f195" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">By running the info method we can find that there are 10 features and no null values and 10 are having type as object and 30 labels are having type as float64 and we can also conclude that below are the features that we need to provide as input to our model. we will later see which feature are important and which features we can ignore some of the feature engineering techniques.</p><h1 id="8c2b" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Features:</h1><p id="2f35" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">1 question_title<br> 2 question_body<br> 3 question_user_name<br> 4 question_user_page<br> 5 answer<br> 6 answer_user_name<br> 7 answer_user_page<br> 8 url<br> 9 category<br> 10 host</p><p id="594c" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">In the above 41 columns, 10 are feature and 30 are the class labels and one column qa_id is the unique ID for every instance.</p><ul class=""><li id="fb54" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">21 class</strong> labels are for <strong class="ky hp">questions</strong> that is the label that starts with “question_…”</li><li id="0558" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">9 class</strong> labels are for <strong class="ky hp">answers</strong> that is the label which starts with “answer_…”</li><li id="a334" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Total we have <strong class="ky hp">30 Class Lables.</strong></li><li id="321e" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">From the 10 features <code class="ho vi vj vk vl b">question_title</code> , <code class="ho vi vj vk vl b">question_body</code> and <code class="ho vi vj vk vl b">answer</code> contain text and the labels are mostly depend on this three features as humans manually labeled based on the these text data.</li></ul><p id="3081" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Now lets check density of words &amp; characters present in the <code class="ho vi vj vk vl b">question_title</code> feature.</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="6e7b" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Running the above code we get the below plots and from the plots we can observe that Both train and test having the same distribution of characters and words.</p><ul class=""><li id="21ce" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph="">Most of the words lies in range 5–10 both train and test.</li><li id="607a" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Most of the characters lies in the range 40–60 train and test.</li></ul><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm vs"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*4qQnaEONkArY_ZjSXqo2Sw.png" loading="lazy" role="presentation" width="700" height="216"></div></div></figure><p id="587e" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Similarly we will check the word and character count for <strong class="ky hp">question_body</strong> and <strong class="ky hp">answer </strong>features. we can use above code snippet for both question_body and answer by just replacing question_title with respective feature will get the below plots.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm vt"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*k2jh7tBNhrxLecppefJ0Fg.png" loading="lazy" role="presentation" width="700" height="211"></div></div></figure><p id="bfdb" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">From <code class="ho vi vj vk vl b"><strong class="ky hp">question_body</strong></code> feature word &amp; character count we can observe that the distribution of both words and characters are very much right skewed.</p><ul class=""><li id="fdba" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph="">Most of the characters in question_body lies below 2500.</li><li id="4bf5" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Most of the words in question_body lies below 1000.</li></ul><p id="a499" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Similarly we will check for <code class="ho vi vj vk vl b"><strong class="ky hp">answer</strong></code><strong class="ky hp"> </strong>feature</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm vu"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*0VdVubRSQwsetvUSzinapw.png" loading="lazy" role="presentation" width="700" height="230"></div></div></figure><ul class=""><li id="d0dd" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph="">As similar to question_body we can find that answer distribution is also skewed.</li><li id="4eed" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Their may be some extreme outlier instance that words/char length are very high in both question_body and answer features.</li></ul><p id="23ac" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Lets analyze <code class="ho vi vj vk vl b">question_body</code> and <code class="ho vi vj vk vl b">answer</code> features sequence length as we have observed skewness.</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="21f4" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">By running the above code we get percentile range from 0–100, 90–100 and 99 -100 having 10 steps per each so that we can get how much percentage of word count are present in the question_body feature.</p><pre class="uf ug uh ui qa vv vw vx"><span id="b44e" class="fl vy sr rf vl b do vz wa l wb" data-selectable-paragraph="">0th percentile of question_body input sequence 1.0<br>10th percentile of question_body input sequence 34.0<br>20th percentile of question_body input sequence 48.0<br>30th percentile of question_body input sequence 61.0<br>40th percentile of question_body input sequence 76.0<br>50th percentile of question_body input sequence 93.0<br>60th percentile of question_body input sequence 116.0<br>70th percentile of question_body input sequence 143.0<br>80th percentile of question_body input sequence 192.0<br>90th percentile of question_body input sequence 282.1999999999998<br>100th percentile of question_body input sequence 4665.0<br><br>90th percentile of question_body input sequence 282.1999999999998<br>91th percentile of question_body input sequence 300.0<br>92th percentile of question_body input sequence 318.0<br>93th percentile of question_body input sequence 342.0<br>94th percentile of question_body input sequence 371.0<br>95th percentile of question_body input sequence 433.0<br>96th percentile of question_body input sequence 499.0<br>97th percentile of question_body input sequence 578.0<br>98th percentile of question_body input sequence 722.0<br>99th percentile of question_body input sequence 1026.7200000000066<br>100th percentile of question_body input sequence 4665.0<br><br>99.1th percentile of question_body input sequence 1087.5959999999995<br>99.2th percentile of question_body input sequence 1170.0<br>99.3th percentile of question_body input sequence 1195.9079999999994<br>99.4th percentile of question_body input sequence 1337.580000000069<br>99.5th percentile of question_body input sequence 1486.0<br>99.6th percentile of question_body input sequence 1580.0<br>99.7th percentile of question_body input sequence 1811.0<br>99.8th percentile of question_body input sequence 1942.0520000000042<br>99.9th percentile of question_body input sequence 3216.0800000000672<br>100th percentile of question_body input sequence 4665.0</span></pre><p id="fc65" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Above is the output of the cell and we can observe that 90 percentage of the question_body text having word count less than 1026 and by observing the percentile values range from 90 to 100. 99% of the question_body text word count less than 1026 so only 99 to 100 that is only 1% of the question_title having word length more than 1026.</p><blockquote class="vb vc vd"><p id="f550" class="uj uk ve ky b tf uw hr ul th ux hv um vf uy uo up vg uz ur us vh va uu uv tp jy fl" data-selectable-paragraph=""><strong class="ky hp">99.8% the of words in question body lies below 1942</strong></p></blockquote><p id="2c6c" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Similarly we will check for <code class="ho vi vj vk vl b"><strong class="ky hp">answer</strong></code><strong class="ky hp"> </strong>feature we can use the same above code snippet just replace question_body with answer will get below output</p><pre class="uf ug uh ui qa vv vw vx"><span id="7f6c" class="fl vy sr rf vl b do vz wa l wb" data-selectable-paragraph="">0th percentile of answer input sequence 2.0<br>10th percentile of answer input sequence 26.0<br>20th percentile of answer input sequence 40.0<br>30th percentile of answer input sequence 55.0<br>40th percentile of answer input sequence 72.0<br>50th percentile of answer input sequence 91.0<br>60th percentile of answer input sequence 114.0<br>70th percentile of answer input sequence 149.0<br>80th percentile of answer input sequence 196.0<br>90th percentile of answer input sequence 293.0<br>100th percentile of answer input sequence 8158.0<br><br>90th percentile of answer input sequence 293.0<br>91th percentile of answer input sequence 312.9800000000005<br>92th percentile of answer input sequence 340.0<br>93th percentile of answer input sequence 363.0799999999999<br>94th percentile of answer input sequence 392.0<br>95th percentile of answer input sequence 428.0<br>96th percentile of answer input sequence 488.28000000000065<br>97th percentile of answer input sequence 548.6599999999999<br>98th percentile of answer input sequence 620.4399999999996<br>99th percentile of answer input sequence 880.880000000001<br>100th percentile of answer input sequence 8158.0<br><br>99.1th percentile of answer input sequence 916.5959999999995<br>99.2th percentile of answer input sequence 956.2560000000012<br>99.3th percentile of answer input sequence 1035.6319999999978<br>99.4th percentile of answer input sequence 1108.044000000018<br>99.5th percentile of answer input sequence 1157.8799999999974<br>99.6th percentile of answer input sequence 1213.8160000000007<br>99.7th percentile of answer input sequence 1338.7879999999932<br>99.8th percentile of answer input sequence 1679.8920000000098<br>99.9th percentile of answer input sequence 2122.1140000000178<br>100th percentile of answer input sequence 8158.0</span></pre><blockquote class="vb vc vd"><p id="3c24" class="uj uk ve ky b tf uw hr ul th ux hv um vf uy uo up vg uz ur us vh va uu uv tp jy fl" data-selectable-paragraph=""><strong class="ky hp">from above output we can observe that only 99.9% of words in answer feature lies below 2200</strong></p></blockquote><p id="1b83" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Still now we have analyzed question_title , question_body and answer feature lets analyze other features as well lets start with <strong class="ky hp">category </strong>feature.</p><h1 id="67cf" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Analyzing <code class="ho vi vj vk vl b">category</code> Feature</h1><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div class="jl jm wc"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/414/1*l8Ko7p26Op8DooXH_rdsGg.png" loading="lazy" role="presentation" width="331" height="456"></div></figure><p id="3667" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">So we have only five different category from those five category most of the question and answer are from technology in both train and test so the distribution of category of question in train and test are also similar.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm wd"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*jApuo3IgkXUrFe5YzRkANg.png" loading="lazy" role="presentation" width="700" height="428"></div></div></figure><p id="e99b" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Also by checking the each of the different category question and answer text we can observe below insights</p><ul class=""><li id="797e" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph="">Five unique category are present in the category feature.</li><li id="b019" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">Technology</strong> and <strong class="ky hp">Stackoverflow</strong> are the highest count and both are related topics.</li><li id="2bbc" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">Life_arts</strong> as the lowest count category.</li><li id="3136" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Distribution of train and test category are the same.</li><li id="0719" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">Life_arts &amp; culture</strong> follow general english syntax &amp; structure.</li><li id="c28d" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">Science</strong> utilizes latex with expressions prepended and appended with symbol: $</li><li id="6984" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">Technology &amp; stackoverflow</strong> have code snippets &amp; logs.</li></ul><h2 id="5485" class="vy sr rf bn ss ke we kf ht ki wf kj hx tj wg wh ib tl wi wj if tn wk wl ij wm fl" data-selectable-paragraph="">Lets plot word cloud for three feature and for both train and test</h2><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm wn"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*rnPvNkrxnjYPLdFu2HP3lA.png" loading="lazy" role="presentation" width="700" height="436"></div></div></figure><p id="d862" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">We can observe that some of mostly used words match between train and test set.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div class="jl jm ue"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/625/0*fTNOtSoEXIqweXFa.gif" loading="lazy" role="presentation" width="500" height="280"></div></figure><h1 id="2420" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Analyzing label</h1><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><pre class="uf ug uh ui qa vv vw vx"><span id="0fd5" class="fl vy sr rf vl b do vz wa l wb" data-selectable-paragraph="">question_asker_inten: no. of unique label values: 9<br>question_body_critic: no. of unique label values: 9<br>question_conversatio: no. of unique label values: 5<br>question_expect_shor: no. of unique label values: 5<br>question_fact_seekin: no. of unique label values: 5<br>question_has_commonl: no. of unique label values: 5<br>question_interesting: no. of unique label values: 9<br>question_interesting: no. of unique label values: 9<br>question_multi_inten: no. of unique label values: 5<br>question_not_really_: no. of unique label values: 5<br>question_opinion_see: no. of unique label values: 5<br>question_type_choice: no. of unique label values: 5<br>question_type_compar: no. of unique label values: 5<br>question_type_conseq: no. of unique label values: 5<br>question_type_defini: no. of unique label values: 5<br>question_type_entity: no. of unique label values: 5<br>question_type_instru: no. of unique label values: 5<br>question_type_proced: no. of unique label values: 5<br>question_type_reason: no. of unique label values: 5<br>question_type_spelli: no. of unique label values: 3<br>question_well_writte: no. of unique label values: 9<br>answer_helpful: no. of unique label values: 9<br>answer_level_of_info: no. of unique label values: 9<br>answer_plausible: no. of unique label values: 9<br>answer_relevance: no. of unique label values: 9<br><strong class="vl hp">answer_satisfaction: no. of unique label values: 17</strong><br>answer_type_instruct: no. of unique label values: 5<br>answer_type_procedur: no. of unique label values: 5<br>answer_type_reason_e: no. of unique label values: 5<br>answer_well_written: no. of unique label values: 9</span></pre><p id="38e2" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">By observing the output above we can conclude that</p><ul class=""><li id="82c4" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph="">The output label are regression(real) values but the distribution is not continuous.</li><li id="0d6c" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Except for <code class="ho vi vj vk vl b">answer_satisfaction</code> label rest every label are having unique values some are with 9 unique values and some are of 5 unique values.</li><li id="75e5" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Using this insights we can use post process to get better scoring</li></ul><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div class="jl jm wo"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/723/1*C93mVl6L0uOJ7iEkBBm3Sg.png" loading="lazy" role="presentation" width="578" height="361"></div></figure><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div class="jl jm wp"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/719/1*N0saDK4PNKZVNYCq5PThGQ.png" loading="lazy" role="presentation" width="575" height="323"></div></figure><p id="1976" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">If we plot the bar plot for all our labels it will be very large become very large page so it better to use below kernel link to view the plots by the plot we can observe that <strong class="ky hp">Label values are imbalance</strong> like for some of the label values are having only one values ex: <strong class="ky hp">question_type_spelling</strong>, <strong class="ky hp">question_not_really_question</strong> etc that is the distribution of label are very dissimilar.</p><div class="wq wr pw py ws wt"><a href="https://www.kaggle.com/manikanthgoud/google-quest-challenge-data-analysis?cellIds=44&amp;kernelSessionId=86743704" rel="noopener  ugc nofollow" target="_blank"><div class="jd o jq"><div class="wu o db dz ep wv"><h2 class="bn hp do bp ik ww il im ly io iq lb fl">Google_quest_challenge_Data_analysis</h2><div class="wx l"><h3 class="bn b do bp ik ww il im ly io iq fk">Explore and run machine learning code with Kaggle Notebooks | Using data from Google QUEST Q&amp;A Labeling</h3></div><div class="js l"><p class="bn b lm bp ik ww il im ly io iq fk">www.kaggle.com</p></div></div><div class="wy l"><div class="wz l xa xb xc wy xd sk wt"></div></div></div></a></div><h1 id="4e4e" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">correlation between target variables</h1><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm xe"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*x9Di56WTMujf1ycGt8dy_w.png" loading="lazy" role="presentation" width="700" height="78"></div></div></figure><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div class="jl jm xf"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/871/1*k6Kcjg6k1EB4K-vonHp8Gw.png" loading="lazy" role="presentation" width="697" height="727"></div></figure><p id="a5e2" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">From the above heatmap of correleation we can observe that <code class="ho vi vj vk vl b">answer_helpful</code>, <code class="ho vi vj vk vl b">answer_level_of_information</code>, <code class="ho vi vj vk vl b">answer_plausible</code>, <code class="ho vi vj vk vl b">answer_releveance</code> and <code class="ho vi vj vk vl b">answer_satification</code> have some correlation between them.</p><p id="404e" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Now lets analyze <strong class="ky hp">Host </strong>feature</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm xg"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*1cnhjAvXDTF0mj4gjdP17Q.png" loading="lazy" role="presentation" width="700" height="311"></div></div></figure><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm xh"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*iIzCctbHaqKESE2a8jQ-zg.png" loading="lazy" role="presentation" width="700" height="314"></div></div></figure><ul class=""><li id="01a0" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph="">All question and answer in the data set are extracted from <strong class="ky hp">63 websites</strong>.</li><li id="949b" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Most of the question and answer are from <strong class="ky hp">stackoverflow.com</strong> as we observe from the <code class="ho vi vj vk vl b">category</code> feature analysis that most of the caterogy fall under <strong class="ky hp">technology and stackoverflow</strong>.</li></ul><h1 id="cba8" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Feature engineering:</h1><p id="460c" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">Below are some of the feature engineering technique that we can experiment and verify which feature are important.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm xi"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/0*HXAfbW10WQ4ihAT7.jpg" loading="lazy" role="presentation" width="700" height="659"></div></div></figure><h1 id="ee9d" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Text count based features:</h1><ul class=""><li id="76ec" class="td te rf ky b tf tg th ti tj tk tl tm tn to tp vr tr ts tt fl" data-selectable-paragraph="">Number of characters in the question_title</li><li id="a5d8" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Number of characters in the question_body</li><li id="d2df" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Number of characters in the answer</li><li id="79b0" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Number of words in the question_title</li><li id="06bd" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Number of words in the question_body</li><li id="8bea" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Number of words in the answer</li><li id="1eb9" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Number of unique words in the question_title</li><li id="8c8c" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Number of unique words in the question_body</li><li id="ea2f" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Number of unique words in the answer</li></ul><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><h1 id="99dd" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">TF-IDF based features:</h1><ul class=""><li id="1f4c" class="td te rf ky b tf tg th ti tj tk tl tm tn to tp vr tr ts tt fl" data-selectable-paragraph="">Character Level N-Gram TF-IDF of question_title</li><li id="5c6b" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Character Level N-Gram TF-IDF of question_body</li><li id="3090" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Character Level N-Gram TF-IDF of answer</li><li id="613b" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Word Level N-Gram TF-IDF of question_title</li><li id="7e14" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Word Level N-Gram TF-IDF of question_body</li><li id="a9bb" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Word Level N-Gram TF-IDF of answer</li></ul><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><h1 id="125f" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Web Scraping Features</h1><p id="b34e" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">In train data set we have one feature name URL where it will land to user page by using web scraping we can extract some of the important features like gold, silver, bronze and reputation scores of the user.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm xj"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*xQR6laNJ7nMPFZy1BCaBzg.png" loading="lazy" role="presentation" width="700" height="514"></div></div></figure><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><h1 id="f7e4" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Preprocessing Text Feature</h1><p id="faa1" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">In this Data set we don’t have any Null value present as in the description of kaggle problem it is written there that Null value is not present in the data set.</p><p id="4115" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">When we deal with text, we generally perform some basic cleaning like lower-casing all the words removing special tokens (like ‘%’, ‘$’, ‘#’, etc.),removal of HTML Tag , \r tags, \n (enter) with space Removed all Special Character.</p><p id="702b" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Below are the Code snippet to remove the HTML Tag ,special Character.</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="fbe4" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Sample text answer data before preprocessing</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm xk"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*aFkBo4TEcVtgm75eYlsS_A.png" loading="lazy" role="presentation" width="700" height="303"></div></div></figure><p id="71e1" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">same text after preprocessing</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm xl"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*iT1j_fXR2tajxUrdlLSIRA.png" loading="lazy" role="presentation" width="700" height="245"></div></div></figure></div><div class="o dz oz tz lw ua" role="separator"><span class="ub hk cj uc ud hi"></span><span class="ub hk cj uc ud hi"></span><span class="ub hk cj uc ud"></span></div><div class="jy ra rb rc rd"><h1 id="ef1d" class="sq sr rf bn ss st xm sv ht sw xn sy hx km xo kn ib kq xp kr if ku xq kv ij tc fl" data-selectable-paragraph="">Build models:</h1><p id="0c96" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">Lets start with base model then will try building more complex model. We know that we have three text feature so lets convert the text into vector so that we can feed that to our models.</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="9267" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Using the above code we can store word and its vector having 300 dimension in embedding_index we will be storing all the words and its vector as key and value pair after loading the embedding_index now we will convert our text to tokens using below code snippet</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="885c" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">using above code we convert text to its unique ID and then we are padding the text to have all the text sequence in the same size by adding zeros to its ending similarly we will do the same for other text features.</p><p id="d1ce" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">After this tokenization and padding we will use this vector to convert into glove embedding that is to convert each word in the sequence that is each token into 300 dimension vector so we will get the similar words together and opposite words far away</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="4528" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">by executing the above cell we will get the embedding weight for question_title feature similarly by referring above code we can get embedding weights for question_body and answer features.</p><h1 id="b5ad" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Building Base LSTM model</h1><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="12ea" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">In this model we are using bi-directional LSTM layer for each of the input layer then concatenating all the three layers and passing through some of the dense layer and in ouput layer we have used sigmoid activation function because we need to have probability score of each of label ranges between 0 to 1.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm xr"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*K6Vy9tXVN_8COq0yoonkbw.png" loading="lazy" role="presentation" width="700" height="363"></div></div></figure><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="92c3" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Using the above call back class we will find spearman score for validation data</p><p id="b9b1" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">why we will be using spearman score as metrics?</p><p id="eb41" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">We need to compare the predicted values with true values as per our use case we need to compare how much our predicted and true are similar to each other and spearman uses rank of the data so its a robust measure to measure the similarity of 30 labels form predicted to true values and in kaggle competition spearman is the evaluation metrics.</p><p id="f006" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Using the above model we have achieved spearman score as 0.279374. This score is for only considering three features that is <code class="ho vi vj vk vl b">question_title</code>, <code class="ho vi vj vk vl b">question_body</code> and <code class="ho vi vj vk vl b">answer</code></p><h1 id="862f" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Base LSTM Model + 16 Feature Engineering Features:</h1><p id="b6b9" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">we have our basic three text features that is question_title, question_body, answer apart from these three features now we will experiment with feature engineering features.</p><p id="f75d" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">In this we will provide these below feature engineering features for our model and let see does this effect the model performance.</p><ul class=""><li id="cd24" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">9 Feature engineering features (9 dim)</strong> — question_title_num_chars, question_body_num_chars, answer_num_chars, question_title_num_words, question_body_num_words, answer_num_words, question_title_num_unique_words, question_body_num_unique_words, answer_num_unique_words</li><li id="1085" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">3 TF-IDF features (384 dim)</strong> — TF-IDF quesion_title, TF_IDF quesiton_body, TF-IDF answer.</li><li id="1a83" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">4 Web scraping features (4 dim)</strong> — reputation, gold, silver, bronze.</li></ul><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm xs"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*63_1bOzg_ETrfvkVSUmRkA.png" loading="lazy" role="presentation" width="700" height="240"></div></div></figure><p id="7bc9" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">By using above features we have achieved the score of 0.0564 that is very low value comparing to previous model with simple three text feature so by using all the feature engineering features our model has degrade its performance drastically so in the next experiment we will be removing some of features and check the performance of the model.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div class="jl jm xt"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/450/1*UOKgTXPHg4ksXopXl-fRQg.gif" loading="lazy" role="presentation" width="360" height="235"></div></figure><h1 id="e070" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Base LSTM model applying 13 Feature Engineering features</h1><blockquote class="vb vc vd"><p id="4be4" class="uj uk ve ky b tf uw hr ul th ux hv um vf uy uo up vg uz ur us vh va uu uv tp jy fl" data-selectable-paragraph=""><em class="rf">Removing TF-IDF features as in the above model the performance has decreased comparing to the base model with only three features as tf-idf has more dimensions</em></p></blockquote><p id="56ed" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Now we are experimenting with only below features and check the performance of our model.</p><ul class=""><li id="421a" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">3 text features</strong> — question_title, question_body, answer</li><li id="1959" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">9 Feature engineering features (9 dim)</strong> — question_title_num_chars, question_body_num_chars, answer_num_chars, question_title_num_words, question_body_num_words, answer_num_words, question_title_num_unique_words, question_body_num_unique_words, answer_num_unique_words</li><li id="1d78" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">4 Web scraping features (4 dim)</strong> — reputation, gold, silver, bronze.</li></ul><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm xu"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*sSTg17DftxiVO5I5JAAs3Q.png" loading="lazy" role="presentation" width="700" height="257"></div></div></figure><p id="411a" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">by removing the tf-idf features we are able to achieve the spearman score of -0.00246 which is very low comparing to base with only three features. so lets remove more features and check the model performance.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div class="jl jm xv"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/544/0*53xBUiaNGgiDq2oa.gif" loading="lazy" role="presentation" width="435" height="245"></div></figure><h1 id="93a3" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Base LSTM model applying 9 Feature Engineering features</h1><blockquote class="vb vc vd"><p id="49ba" class="uj uk ve ky b tf uw hr ul th ux hv um vf uy uo up vg uz ur us vh va uu uv tp jy fl" data-selectable-paragraph=""><em class="rf">Removing </em><strong class="ky hp"><em class="rf">TF-IDF features</em></strong><em class="rf"> as in the above model the performance has decreased comparing to the base model with only three features as tf-idf has more dimension.</em></p><p id="b4bc" class="uj uk ve ky b tf uw hr ul th ux hv um vf uy uo up vg uz ur us vh va uu uv tp jy fl" data-selectable-paragraph=""><em class="rf">Removing </em><strong class="ky hp"><em class="rf">Web scraping features</em></strong></p></blockquote><ul class=""><li id="2a31" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">3 text features</strong> — question_title, question_body, answer</li><li id="5993" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">9 Feature engineering features (9 dim)</strong> — question_title_num_chars, question_body_num_chars, answer_num_chars, question_title_num_words, question_body_num_words, answer_num_words, question_title_num_unique_words, question_body_num_unique_words, answer_num_unique_words</li></ul><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm vt"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*bOavKVFOSYCuN-nKUimSdw.png" loading="lazy" role="presentation" width="700" height="319"></div></div></figure><p id="f5f5" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">The model with meta feature engineering has achieved better performance comparing to the TF-IDF, Web scraping features by achieving the height spearman value as <strong class="ky hp">0.2871.</strong></p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div class="jl jm ue"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/625/0*a6unhIyF4SNDikXm.gif" loading="lazy" role="presentation" width="500" height="375"></div></figure><p id="bfbe" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">I have also performed the same experiment having base LSTM model with all feature engineering features with 100 dimensional embedding but the model has very low score so lower dimensional embedding does not work at all.</p><h1 id="954d" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Overview of base model</h1><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm xw"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*FL2nzMyVl01ErgTrdV6qEg.png" loading="lazy" role="presentation" width="700" height="131"></div></div></figure><ul class=""><li id="14d3" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph="">Model with basic three features (question_title, quesiton_body, answer) + Meta features has acheived best performance comparing to the model with TF-IDF and web scraping features.</li><li id="8b4d" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">The best base model has acheived an spearman score of <strong class="ky hp">0.2871</strong>.</li><li id="219b" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">TF-IDF and Web scraping feature are not important to get best performance, meta features are the important features.</li></ul><h1 id="6656" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Universal sentense encoder</h1><p id="5c0d" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">The Universal Sentence Encoder makes getting sentence level embeddings as easy as it has historically been to lookup the embeddings for individual words. The sentence embeddings can then be trivially used to compute sentence level meaning similarity as well as to enable better performance on downstream classification tasks using less supervised training data.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm xx"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*oT034_5xYc5zFLozuta60Q.png" loading="lazy" role="presentation" width="700" height="511"></div></div></figure><p id="611c" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">From the above plot we can observe that similar sentence are grouped together so this can be used in our model to achieve better performance as our model need to find the similarity from question and answers. for more implementation details check out this <a class="au sp" href="https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder" rel="noopener ugc nofollow" target="_blank">link</a></p><p id="7c8e" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">For this we will be using tensor flow universal sentence encoder pre train embedding</p><p id="c539" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Lets load the embedding layer from tensor flow hub</p><pre class="uf ug uh ui qa vv vw vx"><span id="59cb" class="fl vy sr rf vl b do vz wa l wb" data-selectable-paragraph="">module_url = "https://tfhub.dev/google/universal-sentence-encoder/4"<br>uni_sen_embed = hub.load(module_url)</span></pre><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="2c4f" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">If you observe the output for single word we are having embedding size of 512 and for sentence with multiple words we are having same 512 dimension embedding.</p><p id="6339" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Now lets convert our each text features to 512 embedding using below code snippets</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="d956" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">In embeddings_train, embeddings_valid and embeddings_test dictionary we have three features embeddings let stack all these embeddings into numpy array.</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="5898" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">After getting the embedding using universal sentence encoder let build the model.</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div class="jl jm xy"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/810/1*mZoKVMu5PPSYrGfQytYhaQ.png" loading="lazy" role="presentation" width="648" height="701"></div></figure><p id="53f1" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Training the above USE model we have achieved spearman score of <strong class="ky hp">0.33029</strong> which is far better than the base LSTM model and with only with three basic text feature and here we also need to observe that training this took very less time comparing to base model as we already have the embedding and just need to train three dense layers so training one epoch is less than 1 sec so we can train on more epochs. so let perform more experiments on this USE model with our feature engineering feature which has worked with our base LSTM model and some more new features below are the models we are going to build using USE.</p><pre class="uf ug uh ui qa vv vw vx"><span id="fb40" class="fl vy sr rf vl b do vz wa l wb" data-selectable-paragraph="">1. USE + 9 Meta feature<br>2. USE + L2 distance similarity features<br>3. USE + cosine similarity features<br>4. USE + all distance similarity features + 9 meta features</span></pre><p id="0f0d" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">As in the base model experiments we have got better score with meta features comparing to TF-IDF, web scraping features so will be not using TF-IDF and web scraping features here.</p><h1 id="2e3a" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">USE + 9 Meta Features</h1><p id="56e2" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">let build USE + meta feature model there is no complex architecture involved here just adding one new input and concatenating the layers.</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div class="jl jm xz"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/460/1*y1VYjJyG18KIhpx0lzIZwg.png" loading="lazy" role="presentation" width="368" height="601"></div></figure><p id="d636" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Here we have trained it for 100 epoch and observed that even for 100 epochs the model is having smooth loss and validation loss curve its not over fitting the data or deviating the loss curve you can observer the curve in below image and less time to train as well and after 100 epochs a slight deviation from loss and train so it good up to 100 epochs.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div class="jl jm ya"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/664/1*2fF3dO8gbcNJKBzbR0V2SQ.png" loading="lazy" role="presentation" width="531" height="397"></div></figure><p id="7a14" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">With this USE + 9 meta feature we have achieved spearman score of <strong class="ky hp">0.3713</strong> which is better than simple USE + basic features.</p><h1 id="a20a" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">USE + L2 distance similarity + 9 Meta Features Model</h1><p id="4c23" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">The most common distance is the Euclidean one which is defined by</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm yb"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*Jx0ywgfLhFqlm-CiFA42fQ.png" loading="lazy" role="presentation" width="700" height="269"></div></div><figcaption class="sm bm ps jl jm sn so bn b bo bp fk" data-selectable-paragraph=""><a class="au sp" href="https://towardsdatascience.com/player-similarities-interpolation-aecbf6423c72" rel="noopener" target="_blank">Source</a></figcaption></figure><p id="8c63" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">using above euclidean distance we will compute the distance of features as below:</p><pre class="uf ug uh ui qa vv vw vx"><span id="221b" class="fl vy sr rf vl b do vz wa l wb" data-selectable-paragraph="">question_title_USE_embedding - answer_USE_embedding<br>question_body_USE_embedding - answer_USE_embedding<br>question_body_USE_embedding - question_title_USE_embedding </span></pre><p id="eae5" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">These are the three distance measure will be used with three features and feed as input to our USE model.</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="7c89" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">We will be using same model as above and we will be stacking more features computed above using <strong class="ky hp"><em class="ve">np.hstack </em></strong>as in the above code and we will also use 9 meta feature</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="6d9c" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">The score has not improved as much as above USE + 9 Meta features model but the same score has been achieved by both the model but this model took more epochs to reach high score of <strong class="ky hp">0.36124.</strong></p><h1 id="8faf" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">USE + Cosine distance + 9 Meta features</h1><p id="3d13" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">Now lets perform the same experiment but instead of l2 distance we will try with cosine distance</p><p id="0ee1" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph=""><a class="au sp" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">Euclidean distance</a> is like using a ruler to measure the distance. However, choosing this distance is probably not the best option. For example, Ronaldo is close to Messi because they have high ratings in shoots, speed or dribbles. But a young player like Joao Felix who has same profile to Messi will be further away because his attributes are weaker, but in the same proportion as explained in below images</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm yc"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*D0iRwQFI0mORRI3S38f93A.png" loading="lazy" role="presentation" width="700" height="448"></div></div></figure><p id="3e5e" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">This is where <a class="au sp" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">cosine similarity</a> comes. This is a measure of similarity between two vectors that looks at the angle between them.</p><p id="d7a7" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Given two vectors of attributes x and y, the cosine similarity cos(θ), is represented using a dot product and magnitude In below example, the angle between Messi and Joao Felix is smaller than the angle between Messi and Ronaldo. Even though they were further away.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm yd"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*V1mBnFt3X52PX3KhJKMTPQ.png" loading="lazy" role="presentation" width="700" height="481"></div></div></figure><p id="2d07" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Cosine similarity allows us to better capture “style” rather than pure “statistics” attributes.</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="b782" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">The model has achieved an spearman score of <strong class="ky hp">0.371537</strong> which was similar with above two USE models.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div class="jl jm ue"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/625/0*ez3UujKW79gD8tw3.gif" loading="lazy" role="presentation" width="500" height="380"></div><figcaption class="sm bm ps jl jm sn so bn b bo bp fk" data-selectable-paragraph=""><a class="au sp" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fgfycat.com%2Fgifs%2Fsearch%2Ffuturama&amp;psig=AOvVaw2jkbmcLpB0cQG8_VhRpk-N&amp;ust=1646305525732000&amp;source=images&amp;cd=vfe&amp;ved=0CA0Q3YkBahcKEwjAlKSTpKf2AhUAAAAAHQAAAAAQOw" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h1 id="0504" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">USE + All distance features + 9 Meta features</h1><p id="fb8a" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">Lets stack all the distance features that we have used in the above model and experiment with the model and see how the performance</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="6199" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">From this experiment by stacking all the distance embedding features we were able to achieve the maximum score of <strong class="ky hp">0.37261</strong>. this is better than all the model that we have experimented with so using all distance features we were able to achieve better score than the experiments we have done so far.</p><h1 id="4a71" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">BERT (<strong class="ba"><em class="ye">Bidirectional Encoder Representations for Transformers)</em></strong>:</h1><p id="96c1" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">Before dive into the bert model let have quick high level view of what bert is</p><blockquote class="vb vc vd"><p id="9634" class="uj uk ve ky b tf uw hr ul th ux hv um vf uy uo up vg uz ur us vh va uu uv tp jy fl" data-selectable-paragraph=""><em class="rf">BERT is a deep learning model that has given state-of-the-art results on a wide variety of natural language processing tasks. It stands for </em><strong class="ky hp"><em class="rf">Bidirectional Encoder Representations for Transformers</em></strong><em class="rf">. It has been pre-trained on Wikipedia and BooksCorpus and requires (only) task-specific fine-tuning.</em></p></blockquote><p id="70fe" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">It has caused a stir in the Machine Learning community by presenting state-of-the-art results in a wide variety of NLP tasks, including <strong class="ky hp">Question Answering (SQuAD v1.1)</strong>, <strong class="ky hp">Natural Language Inference (MNLI)</strong>, and others.</p><p id="da18" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">It’s not an exaggeration to say that BERT has significantly altered the NLP landscape. Imagine using a single model that is trained on a large unlabelled dataset to achieve State-of-the-Art results on 11 individual NLP tasks. And all of this with little fine-tuning. That’s BERT! It’s a tectonic shift in how we design NLP models.</p><p id="4532" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">BERT has inspired many recent NLP architectures, training approaches and language models, such as Google’s TransformerXL, OpenAI’s GPT-2, XLNet, ERNIE2.0, RoBERTa, etc.</p><p id="dab0" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">It is basically a bunch of Transformer encoders stacked together (not the whole Transformer architecture but just the encoder). The concept of bi directionality is the key differentiator between BERT and its predecessor, OpenAI GPT. BERT is bidirectional because its self-attention layer performs self-attention on both directions.</p><p id="e782" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">There are a few things that we need to know about bert before start experimenting with it.</p><ul class=""><li id="d95a" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph="">First, It’s easy to get that BERT stands for Bidirectional Encoder Representations from Transformers. Each word here has a meaning to it and we will encounter that one by one. For now, <strong class="ky hp">the key takeaway from this line is — BERT is based on the Transformer architecture.</strong></li><li id="a7c8" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Second, BERT is <strong class="ky hp">pre-trained on a large corpus of unlabelled text</strong> including the entire Wikipedia(that’s 2,500 million words!) and Book Corpus (800 million words). This pre training step is really important for BERT’s success. This is because as we train a model on a large text corpus, our model starts to pick up the deeper and intimate understandings of how the language works. This knowledge is the swiss army knife that is useful for almost any NLP task.</li><li id="ca7f" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Third, BERT is a <strong class="ky hp">deeply bidirectional</strong> model. Bidirectional means that BERT learns information from both the left and the right side of a token’s context during the training phase.</li><li id="6c3f" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Fourth, finally the biggest advantage of BERT is it brought about the <strong class="ky hp">ImageNet movement</strong> with it and the most impressive aspect of BERT is that we can fine-tune it by adding just a couple of additional output layers to create state-of-the-art models for a variety of NLP tasks.</li></ul><h1 id="d3f5" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Architecture of BERT</h1><p id="fd32" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">BERT is a multi-layer bidirectional Transformer encoder. There are two models introduced in the paper.</p><ul class=""><li id="8de5" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph="">BERT base — 12 layers (transformer blocks), 12 attention heads, and 110 million parameters.</li><li id="6d71" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">BERT Large — 24 layers, 16 attention heads and, 340 million parameters.</li></ul><p id="da1b" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">For an in-depth understanding of the building blocks of BERT (aka Transformers), you should definitely check <a class="au sp" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">this awesome post</a> — The Illustrated Transformers.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm yf"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*PqfIKffjPounn0hCP6ywfg.png" loading="lazy" role="presentation" width="700" height="396"></div></div><figcaption class="sm bm ps jl jm sn so bn b bo bp fk" data-selectable-paragraph=""><em class="ye">Here’s a representation of BERT Architecture</em></figcaption></figure><h1 id="d9a9" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Preprocessing Text for BERT</h1><p id="19fa" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">The input representation used by BERT is able to represent a single text sentence as well as a pair of sentences (eg., Question, Answering) in a single sequence of tokens.</p><ul class=""><li id="9364" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph="">The first token of every input sequence is the special classification token — <strong class="ky hp">[CLS]</strong>. This token is used in classification tasks as an aggregate of the entire sequence representation. It is ignored in non-classification tasks.</li><li id="83cf" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">For single text sentence tasks, this <strong class="ky hp">[CLS]</strong> token is followed by the WordPiece tokens and the separator token — <strong class="ky hp">[SEP]</strong>.</li></ul><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm yg"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/0*wRgWJamJ5Zy6IGww.png" loading="lazy" role="presentation" width="700" height="97"></div></div></figure><ul class=""><li id="d276" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph="">For sentence pair tasks, the WordPiece tokens of the two sentences are separated by another [SEP] token. This input sequence also ends with the <strong class="ky hp">[SEP]</strong> token.</li><li id="55d3" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar to token/word embeddings with a vocabulary of 2.</li><li id="adf6" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">A positional embedding is also added to each token to indicate its position in the sequence.</li></ul><p id="66bc" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">BERT developers have set a a specific set of rules to represent languages before feeding into the model.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm yh"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/0*LWFT3-2RIM3eHRfr.png" loading="lazy" role="presentation" width="700" height="222"></div></div></figure><p id="5c0f" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">For starters, every input embedding is a combination of 3 embeddings:</p><ul class=""><li id="5b20" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">Position Embeddings</strong>: BERT learns and uses positional embeddings to express the position of words in a sentence. These are added to overcome the limitation of Transformer which, unlike an RNN, is not able to capture “sequence” or “order” information</li><li id="8812" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">Segment Embeddings</strong>: BERT can also take sentence pairs as inputs for tasks (Question-Answering). That’s why it learns a unique embedding for the first and the second sentences to help the model distinguish between them. In the above example, all the tokens marked as EA belong to sentence A (and similarly for EB)</li><li id="03de" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph=""><strong class="ky hp">Token Embeddings</strong>: These are the embeddings learned for the specific token from the WordPiece token vocabulary</li></ul><p id="7b8d" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">For a given token, its input representation is constructed by <strong class="ky hp">summing the corresponding token, segment, and position embeddings</strong>.</p><p id="81d5" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Such a comprehensive embedding scheme contains a lot of useful information for the model.</p><p id="914b" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">These combinations of preprocessing steps make BERT so versatile. This implies that without making any major change in the model’s architecture, we can easily train it on multiple kinds of NLP tasks.</p><p id="0816" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph=""><strong class="ky hp">Tokenization:</strong> BERT uses WordPiece tokenization. The vocabulary is initialized with all the individual characters in the language, and then the most frequent/likely combinations of the existing words in the vocabulary are iteratively added.</p><p id="e749" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Having this above information about bert let fine tune bert model and see the performance on our task. First load the bert model from tensor flow hub</p><pre class="uf ug uh ui qa vv vw vx"><span id="2749" class="fl vy sr rf vl b do vz wa l wb" data-selectable-paragraph="">hub_url_bert = "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3"<br>bert_layer = hub.KerasLayer(hub_url_bert, trainable=False)</span></pre><p id="b7bb" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">After loading the pre trained bert model let get the vocab file and create tokenizer object which will be used to convert sentence to tokens, mask and segments to provide as input to the bert model.</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="3b06" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Now lets transform our input features to bert input features. the below code snippets is used to get input text to <code class="ho vi vj vk vl b">Input Ids</code> , <code class="ho vi vj vk vl b">Input mask</code>, <code class="ho vi vj vk vl b">Input segment</code> for bert</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="8752" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">In the below <code class="ho vi vj vk vl b">_trim_input</code> function:</p><ul class=""><li id="d0f6" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph="">if the input sentence has the number of tokens &gt; 512, the sentence is trimmed down to 512. To trim the number of tokens, 256 tokens from the start and 256 tokens from the end are kept and the remaining tokens are dropped.</li></ul><blockquote class="vb vc vd"><p id="d40c" class="uj uk ve ky b tf uw hr ul th ux hv um vf uy uo up vg uz ur us vh va uu uv tp jy fl" data-selectable-paragraph=""><strong class="ky hp"><em class="rf">Ex.</em></strong><em class="rf"> suppose an answer has 700 tokens, to trim this down to 512, 256 tokens from the beginning are taken and 256 tokens from the end are taken and concatenated to make 512 tokens. The remaining [700-(256+256) = 288] tokens that are in the middle of the answer are dropped.</em></p></blockquote><ul class=""><li id="ac70" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph="">The logic makes sense because in large texts, the beginning part usually describes what the text is all about and the end part describes the conclusion of the text. This is also closely related to the target features that we need to predict.</li></ul><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="7eba" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">In the below <code class="ho vi vj vk vl b">_convert_to_bert_inputs</code> function we concatinate the three text features in to one single features and convert the input to bert compatable inputs.</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="e156" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Transforming train data set that is text features in to bert training data set.</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="d148" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Similarly we will do it for validation data set and test data set. let create bert layer that we will be used in the model to get the embeddings features of the bert.</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="df32" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Using this let create our bert model and train it using the data that we have converted for bert model and check the performance of our model.</p><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="pi jx l"></div></div></figure><p id="a551" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">Fine tuning the bert model we have achieved an score of 0.3913 which highest of all the model that we have experimented with.</p><h1 id="e5f1" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Overview of all the experiments so far:</h1><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm xi"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/0*5sbBdpyYmkgLs0W4.jpg" loading="lazy" role="presentation" width="700" height="350"></div></div><figcaption class="sm bm ps jl jm sn so bn b bo bp fk" data-selectable-paragraph=""><a class="au sp" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fscreenrant.com%2Frick-morty-science-real-examples%2F&amp;psig=AOvVaw3Heyz73gZA3fV_C5gKgQbH&amp;ust=1646303397790000&amp;source=images&amp;cd=vfe&amp;ved=0CA0Q3YkBahcKEwjAwbOhnKf2AhUAAAAAHQAAAAAQAw" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="da58" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">So far we have experimented with five different base model and five different universal sentence encoder model.</p><ul class=""><li id="4381" class="td te rf ky b tf uw th ux tj vo tl vp tn vq tp vr tr ts tt fl" data-selectable-paragraph="">Bi directional LSTM model has achieved better performance with three text features and 9 meta features.</li><li id="6daa" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">The given data is very less with only 6K points as the training instances and training neural network requires more data to extract pattern and features from the data we need more data so we have used<strong class="ky hp"> transfer learning techniques.</strong></li><li id="e4f0" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">USE has achieved best performance having spearman score of 0.37261 with text features and two distance features and 9 meta features.</li><li id="dcae" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">TF-IDF and web scraping features does not improve the model performance.</li><li id="e317" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">USE has very less time to train and more epochs to fit the data and achieve best score.</li><li id="40ca" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp vr tr ts tt fl" data-selectable-paragraph="">Fine tuning the bert model has achieved high spearman score of 0.3912 so far the experiments we have performed.</li></ul><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm yi"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*8Pouktnp0kT7KxyVoyWeJA.png" loading="lazy" role="presentation" width="700" height="200"></div></div></figure><h1 id="15e6" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Kaggle submission:</h1><p id="8b5b" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">If you are submitting in kaggle make sure to execute your notebook in offline then submit your final best solution notebook.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm yj"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*a-_xhpvx2mPq0tFzF6kOnQ.png" loading="lazy" role="presentation" width="700" height="325"></div></div></figure><p id="ddf3" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">We have achieved a public score of 0.36514 and private score of 0.34118 which is better as some of kernel having 0.44+ public score but very low private score so our model is not over fitting to the train data.</p><h1 id="4386" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Flask demo app:</h1><p id="14b6" class="pw-post-body-paragraph uj uk rf ky b tf tg hr ul th ti hv um tj un uo up tl uq ur us tn ut uu uv tp jy fl" data-selectable-paragraph="">I have also made a flask web app to demonstrate the model predictions where you need to provide data like question title, question description and its answer and by clicking on submit you will the 30 labels predictions output in horizontal bar plot and in table format please using this <a class="au sp" href="https://drive.google.com/file/d/1TUvGYdHwDb6bKE1F_AAWIXjPvvrqgoEP/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"><em class="ve">link </em></a>to access the web app files download it in your local machine and run app.py file.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm yk"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*rx9hj6rBNiLTafES247Umg.png" loading="lazy" role="presentation" width="700" height="320"></div></div></figure><p id="4de1" class="pw-post-body-paragraph uj uk rf ky b tf uw hr ul th ux hv um tj uy uo up tl uz ur us tn va uu uv tp jy fl" data-selectable-paragraph="">After providing the input data that is question title, question body and answer you will get the below web page where you can see the question-answer similarities.</p><figure class="uf ug uh ui qa sf jl jm paragraph-image"><div role="button" tabindex="0" class="sg sh dq si cf sj"><div class="jl jm yl"><img alt="" class="cf sk sl" src="https://miro.medium.com/max/875/1*-HbC7ltMd1hBkYnkSixvOw.png" loading="lazy" role="presentation" width="700" height="1465"></div></div></figure><figure class="uf ug uh ui qa sf"><div class="m l dq"><div class="ym jx l"></div></div></figure><h1 id="24f5" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">Future work</h1><ul class=""><li id="776a" class="td te rf ky b tf tg th ti tj tk tl tm tn to tp vr tr ts tt fl" data-selectable-paragraph="">We have experimented with only one transformed based model bert but we can have more experiments on similar transformed based model like roberta, XLnet, Albert etc and ensembles of the transformer based models.</li></ul><h1 id="3d49" class="sq sr rf bn ss st su sv ht sw sx sy hx km sz kn ib kq ta kr if ku tb kv ij tc fl" data-selectable-paragraph="">References:</h1><h2 id="3d5e" class="vy sr rf bn ss ke we kf ht ki wf kj hx tj wg wh ib tl wi wj if tn wk wl ij wm fl" data-selectable-paragraph="">Data Analysis:</h2><ol class=""><li id="45b2" class="td te rf ky b tf tg th ti tj tk tl tm tn to tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe</a></li><li id="ce31" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="https://www.kaggle.com/mobassir/jigsaw-google-q-a-eda" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/mobassir/jigsaw-google-q-a-eda</a></li></ol><h2 id="c7e1" class="vy sr rf bn ss ke we kf ht ki wf kj hx tj wg wh ib tl wi wj if tn wk wl ij wm fl" data-selectable-paragraph="">Feature engineering:</h2><ol class=""><li id="2359" class="td te rf ky b tf tg th ti tj tk tl tm tn to tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="https://www.kaggle.com/c/google-quest-challenge/discussion/130041" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/google-quest-challenge/discussion/130041</a> — meta features.</li><li id="eca5" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe?scriptVersionId=25618132&amp;cellId=65" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe?scriptVersionId=25618132&amp;cellId=65</a> — tfidf, count based features.</li><li id="d497" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="https://towardsdatascience.com/hands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb" rel="noopener" target="_blank">https://towardsdatascience.com/hands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb</a> — web scraping features</li></ol><h2 id="e01d" class="vy sr rf bn ss ke we kf ht ki wf kj hx tj wg wh ib tl wi wj if tn wk wl ij wm fl" data-selectable-paragraph="">Universal Sentence encoder:</h2><ol class=""><li id="060f" class="td te rf ky b tf tg th ti tj tk tl tm tn to tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="https://www.kaggle.com/abazdyrev/use-features-oof" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/abazdyrev/use-features-oof</a></li><li id="8222" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiv8qzn5aL2AhXCB94KHeKGAjIQFnoECAQQAQ&amp;url=https%3A%2F%2Fwww.tensorflow.org%2Fhub%2Ftutorials%2Fsemantic_similarity_with_tf_hub_universal_encoder&amp;usg=AOvVaw2Al3MpaY_pfxcNslvWnpuu" rel="noopener ugc nofollow" target="_blank">https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiv8qzn5aL2AhXCB94KHeKGAjIQFnoECAQQAQ&amp;url=https%3A%2F%2Fwww.tensorflow.org%2Fhub%2Ftutorials%2Fsemantic_similarity_with_tf_hub_universal_encoder&amp;usg=AOvVaw2Al3MpaY_pfxcNslvWnpuu</a></li><li id="de5f" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiv8qzn5aL2AhXCB94KHeKGAjIQFnoECAMQAQ&amp;url=http%3A%2F%2Fai.googleblog.com%2F2019%2F07%2Fmultilingual-universal-sentence-encoder.html&amp;usg=AOvVaw0l3X0-5ZuOXwgw6b_o5lQa" rel="noopener ugc nofollow" target="_blank">https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiv8qzn5aL2AhXCB94KHeKGAjIQFnoECAMQAQ&amp;url=http%3A%2F%2Fai.googleblog.com%2F2019%2F07%2Fmultilingual-universal-sentence-encoder.html&amp;usg=AOvVaw0l3X0-5ZuOXwgw6b_o5lQa</a></li><li id="3cbe" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwi_0bb15aL2AhUOEIgKHYtwC1sQFnoECAUQAQ&amp;url=https%3A%2F%2Fgaurav5430.medium.com%2Funiversal-sentence-encoding-7d440fd3c7c7&amp;usg=AOvVaw33W-YC1OAfLAkTY5uxJ3Nm" rel="noopener ugc nofollow" target="_blank">https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwi_0bb15aL2AhUOEIgKHYtwC1sQFnoECAUQAQ&amp;url=https%3A%2F%2Fgaurav5430.medium.com%2Funiversal-sentence-encoding-7d440fd3c7c7&amp;usg=AOvVaw33W-YC1OAfLAkTY5uxJ3Nm</a></li></ol><h2 id="3305" class="vy sr rf bn ss ke we kf ht ki wf kj hx tj wg wh ib tl wi wj if tn wk wl ij wm fl" data-selectable-paragraph="">BERT References:</h2><ol class=""><li id="a478" class="td te rf ky b tf tg th ti tj tk tl tm tn to tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03" rel="noopener" target="_blank">BERT for Dummies step by step tutorial by Michel Kana</a></li><li id="df04" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/" rel="noopener ugc nofollow" target="_blank">Demystifying BERT: Groundbreaking NLP Framework by Mohd Sanad Zaki Rizvi</a></li><li id="258f" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" rel="noopener ugc nofollow" target="_blank">A visual guide to using BERT by Jay Alammar</a></li><li id="d19b" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="https://mccormickml.com/2019/07/22/BERT-fine-tuning/" rel="noopener ugc nofollow" target="_blank">BERT Fine tuning By Chris McCormick and Nick Ryan</a></li><li id="e020" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="https://www.reddit.com/r/MachineLearning/comments/ao23cp/p_how_to_use_bert_in_kaggle_competitions_a/" rel="noopener ugc nofollow" target="_blank">How to use BERT in Kaggle competitions — Reddit Thread</a></li><li id="7b1a" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">BERT GitHub repository</a></li><li id="8424" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="https://www.kdnuggets.com/2018/12/bert-sota-nlp-model-explained.html" rel="noopener ugc nofollow" target="_blank">BERT — SOTA NLP model Explained by Rani Horev</a></li><li id="c2b6" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="https://www.youtube.com/watch?v=BhlOGGzC0Q0" rel="noopener ugc nofollow" target="_blank">YOUTUBE — BERT Pretranied Deep Bidirectional Transformers for Language Understanding algorithm by Danny Luo</a></li><li id="b9a1" class="td te rf ky b tf tu th tv tj tw tl tx tn ty tp tq tr ts tt fl" data-selectable-paragraph=""><a class="au sp" href="https://blog.insightdatascience.com/using-bert-for-state-of-the-art-pre-training-for-natural-language-processing-1d87142c29e7" rel="noopener ugc nofollow" target="_blank">State-of-the-art pre-training for natural language processing with BERT by Javed Quadrud-Din</a></li></ol></div></div></section></div></div>