---
layout: post
title: "What is the use of data standardization and where do we use it in machine learning"
featured-img: bound
---


<div><div class="fm as il im in io"></div><div class="ip iq ir is it"><div class=""><h1 id="09e4" class="pw-post-title iu iv iw bn ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js fy" data-selectable-paragraph="">What is the use of data standardization and where do we use it in machine learning</h1></div><p id="c1b7" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph=""><span class="vv dq vw sn"><span class="l"><span class="fm as aq vx vy ou vz"><div><button class="au dg aw ax ay az ba bb bc ph"><svg width="19" height="19" aria-label="View 2 Private Notes"><path d="M14.78 8.07a8.68 8.68 0 0 0-.43-1.38.48.48 0 0 0-.58-.27l-3.12.77V4.03c0-.24-.2-.48-.43-.5a7.23 7.23 0 0 0-1.38 0c-.24.02-.43.26-.43.5V7.2L5.3 6.41a.48.48 0 0 0-.58.27c-.18.45-.33.92-.43 1.39-.05.24.1.5.32.58l3.06.75-1.98 2.96c-.14.2-.13.5.04.67.34.33.7.63 1.1.9.2.13.48.07.62-.12l2.1-3.12 2.08 3.12c.15.19.43.25.63.11a7.7 7.7 0 0 0 1.1-.89.53.53 0 0 0 .03-.67L11.4 9.41l3.06-.76a.52.52 0 0 0 .32-.58" fill-rule="evenodd"></path></svg></button></div></span></span></span>In the process of learning machine learning you will encounter the word standardization, column standardization or mean centering plus scaling but what is purpose of importing standard scalar as below</p><p id="e4a0" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph=""><code class="fn kr ks kt ku b">from sklearn.preprocessing import StandardScalar</code></p><p id="802d" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">What exactly is standardize??</p><p id="d54a" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">How to standardize and why to do so the data before fitting a machine learning model?</p><p id="7608" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">What to standardize in the first place?</p><p id="708b" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">In this post you will get to know the clear insights about standardization so lets get started.</p><p id="6f1f" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">Standardization process comes in while data preprocessing step. Before learning Standardization you also need to know about normalization.</p><p id="cb0b" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph=""><strong class="jv ix">Normalization:</strong></p><p id="e564" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">In column normalization we tool column values and compressed it in range between [0, 1] so as to get rid of scales of each features.</p><p id="d378" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">Ex: lets consider we have dataset having features such as height and weight of a person these two features have different scales values such as height in cms and weight in kgs.</p><p id="8673" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">To get rid of such scales of features we go with normalization, similar to normalization we have ‚Äú<strong class="jv ix">standardization</strong>‚Äù</p><p id="50ad" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph=""><strong class="jv ix">Note: </strong>In practice column standardization is more often used then normalization.</p><p id="bee0" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">Suppose our dataset matrix be like</p><figure class="kw kx ky kz gv la gj gk paragraph-image"><div role="button" tabindex="0" class="lb lc dq ld cf le"><div class="gj gk kv"><img alt="" class="cf lf lg" src="https://miro.medium.com/max/1400/1*XUGnEv9Ul3Zul8Ls2qidXA.png" loading="lazy" role="presentation" width="700" height="389"></div></div></figure><p id="d36a" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">where f1, f2, f3, .. fj,‚Ä¶fd. are d features and 1,2,3,‚Ä¶n are the features/row values.</p><p id="1e7b" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">Consider for one column a1,a2,a3,‚Ä¶an. are ‚Äôn‚Äô values of features ‚Äòfj‚Äô</p><figure class="kw kx ky kz gv la gj gk paragraph-image"><div role="button" tabindex="0" class="lb lc dq ld cf le"><div class="gj gk lh"><img alt="" class="cf lf lg" src="https://miro.medium.com/max/1400/1*CBXz96X8b03cbZuoAQg7sQ.png" loading="lazy" role="presentation" width="700" height="581"></div></div></figure><p id="025b" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">what column standardize does exactly is it converts your column values (a1, a2, a3, ‚Ä¶an) to (a‚Äô1, a‚Äô2, a‚Äô3‚Ä¶..a‚Äôn) such that the mean of transformed data is zero and standard deviation as one. where as in column normalization we transform {a‚Äô1, a‚Äô2, a‚Äô3‚Ä¶..a‚Äôn} all values in range from [0 to 1].</p><p id="67e1" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">a1,a2,a3,‚Ä¶.,an can be of any distribution before transformation but after transformation {a‚Äô1, a‚Äô2, a‚Äô3‚Ä¶..a‚Äôn} will have mean = 0, std-dev =1.</p><h2 id="1b75" class="li lj iw bn lk ll lm ln lo lp lq lr ls ke lt lu lv ki lw lx ly km lz ma mb mc fy" data-selectable-paragraph=""><strong class="ba">But why this transformation is important at all?</strong></h2><p id="7980" class="pw-post-body-paragraph jt ju iw jv b jw md jy jz ka me kc kd ke mf kg kh ki mg kk kl km mh ko kp kq ip fy" data-selectable-paragraph="">Lets understand its important in geometry perspective.</p><figure class="kw kx ky kz gv la gj gk paragraph-image"><div role="button" tabindex="0" class="lb lc dq ld cf le"><div class="gj gk mi"><img alt="" class="cf lf lg" src="https://miro.medium.com/max/1400/1*UyHNV_ECLdN39KyD-U0IeA.jpeg" loading="lazy" role="presentation" width="700" height="562"></div></div></figure><p id="36a6" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">Consider below dataset, where ‚Äò<strong class="jv ix">x‚Äô </strong>is the height of the person and <strong class="jv ix">‚Äòy‚Äô </strong>is the weight of the person and <strong class="jv ix">mean</strong> lies in the middle of the data set as show below and <strong class="jv ix">variance</strong> as well.</p><p id="c2ca" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">Variance measure the spread of the data, more the spread ‚Üí more the variance value.</p><p id="7f7d" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">By transforming the above data set using column standardization we get mean at the origin as show below and we have pushed point closer such that its spread/standard deviation =1. If the standard deviation before transformation is less than 1 then after transformation we will be pulling far away from each other to achieve standard deviation to 1.</p><figure class="kw kx ky kz gv la gj gk paragraph-image"><div role="button" tabindex="0" class="lb lc dq ld cf le"><div class="gj gk mj"><img alt="" class="cf lf lg" src="https://miro.medium.com/max/1400/1*Mlayh3pNGSBbhF0LIxfwWQ.jpeg" loading="lazy" role="presentation" width="700" height="523"></div></div></figure><p id="1ca7" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">Column standardization is also called as mean centering plus scaling.</p><figure class="kw kx ky kz gv la gj gk paragraph-image"><div role="button" tabindex="0" class="lb lc dq ld cf le"><div class="gj gk mk"><img alt="" class="cf lf lg" src="https://miro.medium.com/max/1400/1*cVZfn6rTKsOqFHyIjaCpCg.jpeg" loading="lazy" role="presentation" width="700" height="978"></div></div></figure><h2 id="3e0e" class="li lj iw bn lk ll lm ln lo lp lq lr ls ke lt lu lv ki lw lx ly km lz ma mb mc fy" data-selectable-paragraph="">If we don‚Äôt do standardization does it affect anything?</h2><p id="4d40" class="pw-post-body-paragraph jt ju iw jv b jw md jy jz ka me kc kd ke mf kg kh ki mg kk kl km mh ko kp kq ip fy" data-selectable-paragraph="">if the features follow different scaling, then it would be difficult for the model to converge faster and the computation time of the training increases and also it doesn‚Äôt yield better results.</p><p id="576c" class="pw-post-body-paragraph jt ju iw jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ip fy" data-selectable-paragraph="">Column standardization is more effective when the underlying distribution is Gaussian with say mean=mu and std-dev=sigma. This is due to the fact that when you normalize such a feature, you get the new feature to be of N(0,1) distribution. This is an ideal distribution with lots of ML, Stats and Optimization techniques assuming a Gaussian distribution to make the proofs work beautifully. <br> <br> That doesn‚Äôt mean standardization is not good for no-gaussian distributed features. It still results in a new feature with a mean of 0 and variance of 1, but not N(0,1) distribution. In practice, we perform standardization irrespective of the underlying feature distribution. But the mathematical proofs are well suited when the distribution in Gaussian.</p><h1 id="999a" class="ml lj iw bn lk mm mn mo lo mp mq mr ls ms mt mu lv mv mw mx ly my mz na mb nb fy" data-selectable-paragraph="">Summary:</h1><ol class=""><li id="832b" class="nc nd iw jv b jw md ka me ke ne ki nf km ng kq nh ni nj nk fy" data-selectable-paragraph=""><em class="nl">Data standardization is the process of rescaling the attributes so that they have mean as 0 and variance as 1.</em></li><li id="00a2" class="nc nd iw jv b jw nm ka nn ke no ki np km nq kq nh ni nj nk fy" data-selectable-paragraph=""><mark class="wa wb ph"><em class="nl">The ultimate goal to perform standardization is to bring down all the features to a common scale without distorting the differences in the range of the values.</em></mark></li><li id="3720" class="nc nd iw jv b jw nm ka nn ke no ki np km nq kq nh ni nj nk fy" data-selectable-paragraph=""><mark class="wa wb ph"><em class="nl">In sklearn.preprocessing.StandardScaler(), centering and scaling happens independently on each feature.</em></mark></li></ol></div><div class="o dz nr ns ib nt" role="separator"><span class="nu fj cj nv nw nx"></span><span class="nu fj cj nv nw nx"></span><span class="nu fj cj nv nw"></span></div><div class="ip iq ir is it"><h1 id="326b" class="ml lj iw bn lk mm ny mo lo mp nz mr ls ms oa mu lv mv ob mx ly my oc na mb nb fy" data-selectable-paragraph="">If you have any feedback or critiques, please feel free to share them with me. If this walkthrough helped you, please like üëè the article. Cheers!</h1></div></div>