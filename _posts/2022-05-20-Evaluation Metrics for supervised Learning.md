---
layout: post
title: "Evaluation Metrics for supervised Learning"
featured-img: shane-rounce-205187
---

<div><div class="hh as iw ix iy iz"></div><div class="ja jb jc jd je"><div class=""><h1 id="7219" class="pw-post-title jf jg jh bn ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd fn" data-selectable-paragraph="">Evaluation Metrics for supervised Learning</h1></div><figure class="gs gu kf kg kh ki go gp paragraph-image"><div class="go gp ke"><img alt="" class="cf kj kk" src="https://miro.medium.com/max/803/0*3c9tzGKFkvegw_uW.jpeg" loading="lazy" role="presentation" width="642" height="389"></div><figcaption class="kl bm gq go gp km kn bn b bo bp fm" data-selectable-paragraph="">Source: <a class="au ko" href="https://andronikmk.medium.com/machine-learning-evaluation-metrics-a-guided-tour-a4218d640148" rel="noopener">https://andronikmk.medium.com/machine-learning-evaluation-metrics-a-guided-tour-a4218d640148</a></figcaption></figure><p id="3d57" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph=""><span class="yb dq yc ol"><span class="l"><span class="hh as aq yd ye qr yf"><div><button class="au fq aw ax ay az ba bb bc re"><svg width="19" height="19" aria-label="View 1 Private Notes"><path d="M14.78 8.07a8.68 8.68 0 0 0-.43-1.38.48.48 0 0 0-.58-.27l-3.12.77V4.03c0-.24-.2-.48-.43-.5a7.23 7.23 0 0 0-1.38 0c-.24.02-.43.26-.43.5V7.2L5.3 6.41a.48.48 0 0 0-.58.27c-.18.45-.33.92-.43 1.39-.05.24.1.5.32.58l3.06.75-1.98 2.96c-.14.2-.13.5.04.67.34.33.7.63 1.1.9.2.13.48.07.62-.12l2.1-3.12 2.08 3.12c.15.19.43.25.63.11a7.7 7.7 0 0 0 1.1-.89.53.53 0 0 0 .03-.67L11.4 9.41l3.06-.76a.52.52 0 0 0 .32-.58" fill-rule="evenodd"></path></svg></button></div></span></span></span>In real world machine learning we will encounter a lot of different evaluation metrics, sometimes we need to create our own custom evaluation metrics based on the problem, But in this notebook we will see some of the most commonly used evaluation metrics.</p><p id="a1ae" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">In <strong class="kr ji">classification</strong> the most common metrics used are:</p><ul class=""><li id="e6c6" class="ln lo jh kr b ks kt kw kx la lp le lq li lr lm ls lt lu lv fn" data-selectable-paragraph="">Accuracy</li><li id="2b36" class="ln lo jh kr b ks lw kw lx la ly le lz li ma lm ls lt lu lv fn" data-selectable-paragraph="">Precision</li><li id="99d0" class="ln lo jh kr b ks lw kw lx la ly le lz li ma lm ls lt lu lv fn" data-selectable-paragraph="">Recall</li><li id="7937" class="ln lo jh kr b ks lw kw lx la ly le lz li ma lm ls lt lu lv fn" data-selectable-paragraph="">F1-Score</li><li id="c386" class="ln lo jh kr b ks lw kw lx la ly le lz li ma lm ls lt lu lv fn" data-selectable-paragraph="">Area under the curve or (area under the ROC(receiver operating characteristic) curve) <em class="mb">AUC</em></li><li id="f8d4" class="ln lo jh kr b ks lw kw lx la ly le lz li ma lm ls lt lu lv fn" data-selectable-paragraph="">Log loss</li></ul><p id="0ace" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Above metrics can also apply for multi class classification. we will see for <strong class="kr ji">binary</strong> and <strong class="kr ji">multi class</strong> level implementation of these metrics in this notebook</p><p id="4ee5" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Before going through this article please refer below kaggle notebook so that you can run the code and edit as per your assumption.</p><div class="mc md gw gy me mf"><a href="https://www.kaggle.com/code/manikanthgoud/evaluation-metrics-for-supervised-learning?kernelSessionId=98866653" rel="noopener  ugc nofollow" target="_blank"><div class="mg o ih"><div class="mh o db dz ep mi"><h2 class="bn ji do bp mj mk ml mm mn mo mp jg fn">Evaluation Metrics for supervised Learning</h2><div class="mq l"><h3 class="bn b do bp mj mk ml mm mn mo mp fm">Explore and run machine learning code with Kaggle Notebooks | Using data from No attached data sources</h3></div><div class="mr l"><p class="bn b hq bp mj mk ml mm mn mo mp fm">www.kaggle.com</p></div></div><div class="ms l"><div class="wz l mu mv mw ms mx kj mf"></div></div></div></a></div><figure class="mz na nb nc ha ki go gp paragraph-image"><div role="button" tabindex="0" class="nd ne dq nf cf ng"><div class="go gp my"><img alt="" class="cf kj kk" src="https://miro.medium.com/max/875/1*mvDSAuBWmLz72pJjluoVOA.png" loading="lazy" role="presentation" width="700" height="43"></div></div></figure><p id="1952" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Let’s first build a basic classification model to apply all our evaluation metrics</p><h1 id="8661" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">Building the Model</h1><p id="c109" class="pw-post-body-paragraph kp kq jh kr b ks of ku kv kw og ky kz la oh lc ld le oi lg lh li oj lk ll lm ja fn" data-selectable-paragraph="">Loading the breast cancer dateset from sklearn</p><figure class="mz na nb nc ha ki"><div class="m l dq"><div class="ok ol l"></div></div></figure><pre class="mz na nb nc ha om fe on"><span id="8735" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph=""># the data is in the type of dict in this cell will check the key <br># and value assoicated with it<br><br>for key in dataset.keys():<br>    print(key)</span><span id="2a6b" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">##### output #######</span><span id="0d23" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">data<br>target<br>frame<br>target_names<br>DESCR<br>feature_names<br>filename</span><span id="c448" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">print("Total no of features: ",len(dataset['feature_names']) ,'\n\n')<br><br>for index, feature in enumerate(dataset['feature_names'], start=1):<br>    print(index, feature)</span><span id="413a" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">Total no of features:  30 <br><br><br>1 mean radius<br>2 mean texture<br>3 mean perimeter<br>4 mean area<br>5 mean smoothness<br>6 mean compactness<br>7 mean concavity<br>8 mean concave points<br>9 mean symmetry<br>10 mean fractal dimension<br>11 radius error<br>12 texture error<br>13 perimeter error<br>14 area error<br>15 smoothness error<br>16 compactness error<br>17 concavity error<br>18 concave points error<br>19 symmetry error<br>20 fractal dimension error<br>21 worst radius<br>22 worst texture<br>23 worst perimeter<br>24 worst area<br>25 worst smoothness<br>26 worst compactness<br>27 worst concavity<br>28 worst concave points<br>29 worst symmetry<br>30 worst fractal dimension</span><span id="5cb9" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph=""># Loading the dataset in pandas dataframe for analysis the dataset<br><br>data = pd.DataFrame(dataset['data'],  columns= dataset['feature_names'])<br>data['target'] = pd.Series(dataset.target)</span></pre><figure class="mz na nb nc ha ki go gp paragraph-image"><div role="button" tabindex="0" class="nd ne dq nf cf ng"><div class="go gp oy"><img alt="" class="cf kj kk" src="https://miro.medium.com/max/875/1*PwHxlxaNuo1y4zh4AU_nBQ.png" loading="lazy" role="presentation" width="700" height="191"></div></div></figure><pre class="mz na nb nc ha om fe on"><span id="1951" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph=""># to check the shape of the entire dataset<br><br>data.shape</span><span id="ee0c" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">(569, 31)</span><span id="f0c0" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph=""># Check if dataset is blanced or not that is to check if both the class label are partially equal or not <br><br>data.target.value_counts().sort_values().plot(kind = 'bar')<br>print(data.target.value_counts())</span><span id="1725" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">1    357<br>0    212<br>Name: target, dtype: int64</span></pre><figure class="mz na nb nc ha ki go gp paragraph-image"><div class="go gp oz"><img alt="" class="cf kj kk" src="https://miro.medium.com/max/643/1*_RMN1e_uUrGq1hk7PvvREQ.png" loading="lazy" role="presentation" width="514" height="353"></div></figure><p id="6983" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph=""><strong class="kr ji">Observations:</strong> From the above analysis we can conclude that the its a binary classification problem and having 30 features all of float type and we have total 569 instance with no null values. based on the 30 values we need to classify if person is having breast cancer or not</p><pre class="mz na nb nc ha om fe on"><span id="a140" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph="">X = data.drop(['target'],axis=1)<br><br>y = data[['target']]<br><br>X.shape, y.shape</span><span id="6794" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">### output #########</span><span id="3b20" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">((569, 30), (569, 1))</span><span id="f978" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph=""><br>X = data.drop(['target'],axis=1)<br><br>y = data[['target']]<br><br>X.shape, y.shape</span><span id="62dc" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">#### output ######3333</span><span id="af01" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">((569, 30), (569, 1))</span><span id="2156" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph=""># spliting the data into train and test <br>from sklearn.model_selection import train_test_split<br><br>x_train, x_test, y_train,  y_test = train_test_split(X,y, test_size=0.33, random_state=42)</span><span id="f6ad" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">x_train.shape, x_test.shape</span><span id="ce61" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">((381, 30), (188, 30))</span><span id="7cf5" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph=""># Training the model<br><br>from sklearn.neighbors import KNeighborsClassifier<br><br>model = KNeighborsClassifier(n_neighbors=3)<br>model.fit(x_train,y_train.values.ravel())  # .values will give the values in an array. (shape: (n,1) and .ravel will convert that array shape to (n, )<br><br>y_pred = model.predict(x_test)</span><span id="6cef" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">(y_pred)</span><span id="7dc3" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">array([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,<br>       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,<br>       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,<br>       0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,<br>       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,<br>       0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,<br>       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,<br>       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,<br>       0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1])</span><span id="cf11" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">y_test = (y_test.values.ravel())</span></pre><p id="a98a" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Now we have our y_test and y_predicted so we will learn here which metrics should be used based on data and target. Lets start with simple accuracy metrics.</p><h1 id="ce62" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">Accuracy:</h1><p id="23d3" class="pw-post-body-paragraph kp kq jh kr b ks of ku kv kw og ky kz la oh lc ld le oi lg lh li oj lk ll lm ja fn" data-selectable-paragraph="">Accuracy is simple and very basic metrics which commonly used in machine learning. It defines how accurate our model is.</p><p id="fd5d" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Example if we have 100 total test points in that 50 points as postive and 50 as negitive our model classifies 90 data points(Xq or instances) as positive then our model accuracy is 90%. if only 73 points as correct points then its 73% accuracy.</p><p id="3b19" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">It can be also defined as number of correctly classified points to that of total number of points.</p><p id="151c" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Accuracy = No. of correctly classified points / total no. of points</p><h2 id="97b2" class="oo ni jh bn nj pa pb pc nn pd pe pf nr la pg ph nv le pi pj nz li pk pl od pm fn" data-selectable-paragraph="">Custom accuracy funcion</h2><figure class="mz na nb nc ha ki"><div class="m l dq"><div class="ok ol l"></div></div></figure><pre class="mz na nb nc ha om fe on"><span id="7480" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph="">custom_accuracy = accuracy(y_test,y_pred)<br>print(accuracy(y_test,y_pred))</span><span id="0097" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">0.9414893617021277</span><span id="36bc" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">from sklearn.metrics import accuracy_score<br><br>print(accuracy_score(y_test, y_pred))<br>sklearn_accuracy = accuracy_score(y_test, y_pred)</span><span id="a8c9" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">0.9414893617021277</span><span id="d55d" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph=""># Check both custom and sklean accuracy is same or not<br>custom_accuracy == sklearn_accuracy # True means we have not made any mistakes in the implementation.</span><span id="006f" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">True</span></pre><h1 id="67ac" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">Note: we cannot use accuracy as a metrics when we have imbalance data(label skewed distribution).</h1><figure class="mz na nb nc ha ki go gp paragraph-image"><div role="button" tabindex="0" class="nd ne dq nf cf ng"><div class="go gp pn"><img alt="" class="cf kj kk" src="https://miro.medium.com/max/875/1*NNchm-KnBsa8bkozbeeVdg.png" loading="lazy" role="presentation" width="700" height="347"></div></div></figure><p id="8eb5" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">As mentioned above if we use accuracy as the metrics for imbalance data suppose we have 90 positive points and 10 negitive points support our model outputs positive for any query(Xq) or instance point as positive its a very dumb model that what every its input is just return them as positive then also we get the accuracy as 90 as it will predict all the points as positive. so we need to better metrics.</p><h1 id="bbf1" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">Precision:</h1><p id="23bd" class="pw-post-body-paragraph kp kq jh kr b ks of ku kv kw og ky kz la oh lc ld le oi lg lh li oj lk ll lm ja fn" data-selectable-paragraph="">Before learning about precision we need to understand few terms:</p><ul class=""><li id="1e95" class="ln lo jh kr b ks kt kw kx la lp le lq li lr lm ls lt lu lv fn" data-selectable-paragraph=""><strong class="kr ji">True positive (TP)</strong>: Given an instance, if our model predict as positive and the actual value is also positive then it is considered as true positive.</li><li id="db74" class="ln lo jh kr b ks lw kw lx la ly le lz li ma lm ls lt lu lv fn" data-selectable-paragraph=""><strong class="kr ji">True negative (TN)</strong>: Given an instance, if our model predict as negative and the actual value is also negative then it is considered as true negative.</li></ul><blockquote class="po pp pq"><p id="c6ab" class="kp kq mb kr b ks kt ku kv kw kx ky kz pr lb lc ld ps lf lg lh pt lj lk ll lm ja fn" data-selectable-paragraph=""><strong class="kr ji"><em class="jh">In short: If your model correctly predict the positive class then its true positive and if your model correctly predict the negative class as negative then its a true negative</em></strong></p></blockquote><ul class=""><li id="c61a" class="ln lo jh kr b ks kt kw kx la lp le lq li lr lm ls lt lu lv fn" data-selectable-paragraph=""><strong class="kr ji">False positive (FP)</strong>: Given an instance, if our model predict as positive and the actual value is negative then it is considered as false positive.</li><li id="8a58" class="ln lo jh kr b ks lw kw lx la ly le lz li ma lm ls lt lu lv fn" data-selectable-paragraph=""><strong class="kr ji">Flase negative (FN)</strong>: Given an instance, if our model predict as negative and the actual value is positive then its a false negative.</li></ul><blockquote class="po pp pq"><p id="71c4" class="kp kq mb kr b ks kt ku kv kw kx ky kz pr lb lc ld ps lf lg lh pt lj lk ll lm ja fn" data-selectable-paragraph=""><strong class="kr ji"><em class="jh">In short: If you model incorrectly predicts the positive class, it is a false positive, if your model incorrectly predicts negative class then its a false negative.</em></strong></p></blockquote><h2 id="0260" class="oo ni jh bn nj pa pb pc nn pd pe pf nr la pg ph nv le pi pj nz li pk pl od pm fn" data-selectable-paragraph="">Let’s implement all the above four terms:</h2><figure class="mz na nb nc ha ki"><div class="m l dq"><div class="ok ol l"></div></div></figure><h2 id="55fd" class="oo ni jh bn nj pa pb pc nn pd pe pf nr la pg ph nv le pi pj nz li pk pl od pm fn" data-selectable-paragraph="">Note: If you want to describe the accuracy using above terms, we can write it as Accuracy score = (TP+TN)/(TP+TN+FP+FN)</h2><p id="4616" class="pw-post-body-paragraph kp kq jh kr b ks of ku kv kw og ky kz la oh lc ld le oi lg lh li oj lk ll lm ja fn" data-selectable-paragraph="">Lets implement accuray score using TP,TN,FP and FN</p><figure class="mz na nb nc ha ki"><div class="m l dq"><div class="ok ol l"></div></div></figure><p id="8ea0" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">We can quickly check the correctness of this function by comparing it to our previosuly implemented and scikit-learn version accuracy</p><pre class="mz na nb nc ha om fe on"><span id="aade" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph=""># custom accuracy,    verion 2 accuracy using tf/tn/fp/fn,  sklean accuracy<br>accuracy(y_test, y_pred), accuracy_v2(y_test, y_pred), accuracy_score(y_test, y_pred)</span><span id="a6b7" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">(0.9414893617021277, 0.9414893617021277, 0.9414893617021277)</span></pre><h1 id="5480" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">Now, lets start learning about percision</h1><h1 id="bb77" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">Precision = TP/(TP + FP)</h1><p id="958c" class="pw-post-body-paragraph kp kq jh kr b ks of ku kv kw og ky kz la oh lc ld le oi lg lh li oj lk ll lm ja fn" data-selectable-paragraph="">Precision is the ratio of True Positives (TPs) to all the positives predicted by the model (TP + FP).</p><p id="e64a" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Let’s say we make a new model on the new skewed dataset and our model correctly identified 80 non-cancer out of 90 and 8 as cancer out of 10. Thus, we identified 88 correct instances out of 100 successfully. The accuracy is therefore, 0.88 or 88%</p><p id="af54" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">But, out of these 100 samples, 10 non-cancer data instance are misclassified as having cancer and 2 cancer patient are misclassified as not having cancer.</p><p id="5d24" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Thus, we have:</p><ul class=""><li id="468c" class="ln lo jh kr b ks kt kw kx la lp le lq li lr lm ls lt lu lv fn" data-selectable-paragraph="">TP : 8</li><li id="ff81" class="ln lo jh kr b ks lw kw lx la ly le lz li ma lm ls lt lu lv fn" data-selectable-paragraph="">TN : 80</li><li id="b371" class="ln lo jh kr b ks lw kw lx la ly le lz li ma lm ls lt lu lv fn" data-selectable-paragraph="">FP : 10</li><li id="5aad" class="ln lo jh kr b ks lw kw lx la ly le lz li ma lm ls lt lu lv fn" data-selectable-paragraph="">FN : 2</li></ul><p id="51a2" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">so, our precision is 8/(8+10) = 0.444. This means our model is correctly 44.4% times when its trying to identify positive samples</p><p id="3cbb" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Now, as we have implemented TP, TN, FP and FN, we can easily implement precision in python.</p><figure class="mz na nb nc ha ki"><div class="m l dq"><div class="ok ol l"></div></div></figure><pre class="mz na nb nc ha om fe on"><span id="8ba2" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph="">from sklearn.metrics import precision_score<br>precision(y_test, y_pred), precision_score(y_test, y_pred)</span><span id="bc44" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">(0.9583333333333334, 0.9583333333333334)</span></pre><p id="38ca" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">So, we can conclude that our custom precision and sklearn precision are same</p><p id="5f37" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">The higher number of FPs our model predicts, the lower the value of Recall. Mathematically speaking, a larger denominator leads to a smaller fraction.</p><p id="a769" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph=""><strong class="kr ji">Is it a good measure of performance?</strong></p><p id="0cf8" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">The answer again highly depends on the task at hand.</p><p id="5bf8" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Let us take a bit less depressing example, where we want to identify Patient having breast cancer or not.</p><p id="9c43" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">If we misidentify an non-cancer patient as cancer, we increase the False Positive (FP) error. If we misidentify a cancer patient as non-cancer, we increase the False Negative (FN) number.</p><p id="7ec4" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">In this case, the cost of FP is higher than the cost of FN because every FP case will make us miss cancer patient, while we can deal with non-cancer patient by performing more test on the patient but we cannot miss the cancer patient.</p><p id="cc4b" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Hence, in this case, we want to get every cancer patient. In other words, we need to minimize the number of non-cancer patient to be cancer</p><p id="d6c7" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">When we need to be more confident about the True Positives (TP and )minimize the False Positives (FP = 0) and , we aim for a perfect Precision, i.e., Precision = 1 (or 100%).</p><p id="765f" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Precision tells us how precise are we with identifying a specific label. Such as, how many out of those we labeled as ill are actually ill? (Here, ill is Positive confirmation and healthy is Negative confirmation).</p><h1 id="a04b" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">Recall or Sensitivity:</h1><p id="902e" class="pw-post-body-paragraph kp kq jh kr b ks of ku kv kw og ky kz la oh lc ld le oi lg lh li oj lk ll lm ja fn" data-selectable-paragraph="">Recall (or Sensitivity) is the ratio of True Positives (TP) to all the correct predictions in your dataset (TP + FN). False Negatives are included here because they are actually the correct predictions too.</p><h1 id="c4f0" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">Recall = TP/(TP + FN)</h1><figure class="mz na nb nc ha ki"><div class="m l dq"><div class="ok ol l"></div></div></figure><pre class="mz na nb nc ha om fe on"><span id="c327" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph="">from sklearn.metrics import recall_score<br><br>recall(y_test, y_pred), recall_score(y_test,y_pred)</span><span id="9fab" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">(0.9504132231404959, 0.9504132231404959)</span></pre><p id="777f" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">So, from the above we can say that our custom recall and sklearn recall values are same</p><p id="a573" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">The higher number of FNs our model predicts, the lower the value of Recall.</p><p id="b5e5" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph=""><strong class="kr ji">Is it a good measure of performance?</strong></p><p id="82a4" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">The answer again highly depends on the task at hand.</p><p id="2b09" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Let us take an unfortunate but realistic example related with the COVID-19 pandemic:</p><p id="7174" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">If we misidentify affected by the coronavirus patients as healthy, we increase the False Negative (FN) error. If we misidentify healthy people as coronavirus-affected patients, we increase the False Positive (FP) number.</p><p id="073d" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">In this case, the cost of FN is higher than the cost of FP because every FN case will continue spreading the virus around, while the FP will not.</p><p id="a7ab" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Hence, in this case, we need to identify every COVID-19 infection!</p><p id="5c30" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">When we need to eliminate the False Negatives (FN = 0), we aim for a perfect Recall, i.e., Recall = 1 (or 100%). In other words, a highly sensitive model will flag almost everyone who has the disease and not generate many False Negatives.</p><p id="1934" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">For a good model, our precision and recall values should be high. we see that in the above model we have both precision ad recall as high</p><p id="8b0b" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Most of the model predicts a probability, and when we predict, we usually choose this threshold to be 0.5. this threshold is not always ideal, and depending on this threshold, our value of precision and recall can change drastically, if for every threshold we choose, we calculate the precision and recall values, we can create a pot between these sets of values. this plot or curve is known as the precision-recall curve.</p><p id="6a7b" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Before looking into the precision-recall curve, lets assume two lists.</p><pre class="mz na nb nc ha om fe on"><span id="976f" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph="">y_true_1 = [0,1,1,0,0,1,0,0,1,1,1,0,0,0,0,1,0,0,0,1,0]<br>y_pred_1 = [0.25385164, 0.47153354, 0.52181292, 0.74550538, 0.05293734,<br>       0.02329051, 0.78833001, 0.1690708 , 0.70097403, 0.51098548,<br>       0.07442634, 0.55349556, 0.2294754 , 0.37426226, 0.16770641,<br>       0.27747923, 0.82226646, 0.62389247, 0.41162855, 0.01400135,<br>       0.46683325]</span></pre><p id="edae" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">So, y_true is our targets, and y_pred is the probability values for a sample being assigned a value of 1. so, now we look at probabilities in prediction instead of the predicted value (which is most of the time calculated with a threshold at 0.5)</p><pre class="mz na nb nc ha om fe on"><span id="0da4" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph="">precisions = []<br>recalls = []<br># how we calculated threshold will be explained further<br>thresholds = [0.49344837, 0.6820013 , 0.13756598, 0.4702898 , 0.87279533,<br>       0.6297947 , 0.33575179, 0.50324857, 0.1237034 , 0.43555069,<br>       0.66494372, 0.60502148, 0.55022514, 0.57837422]<br></span><span id="38aa" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph=""># for every threshold, calculate predictions in binary<br># and uppend calculated prcision ad recalls<br># to their respective lists<br>for i in thresholds:<br>    temp_prediction = [1 if x &gt;= i else 0 for x in y_pred_1]<br>    p = precision(y_true_1, temp_prediction)<br>    r = recall(y_true_1, temp_prediction)<br>    precisions.append(p)<br>    recalls.append(r)</span></pre><p id="b8e6" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Now, we can plot these values of precisions and recalls</p><pre class="mz na nb nc ha om fe on"><span id="1e63" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph="">import matplotlib.pyplot as plt</span><span id="fb6d" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">plt.figure(figsize=(7,7))<br>plt.plot(sorted(recalls), sorted(precisions))<br>plt.xlabel('Recalls', fontsize=15)<br>plt.ylabel('Precisions', fontsize=15)<br>plt.show();</span></pre><figure class="mz na nb nc ha ki go gp paragraph-image"><div class="go gp pu"><img alt="" class="cf kj kk" src="https://miro.medium.com/max/558/0*NRMAKH3lx5WcELmb.png" loading="lazy" role="presentation" width="446" height="430"></div></figure><p id="22bb" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">This precision-recall curve looks very different from what you might have seen on the internet. It’s because we have taken few instance points.</p><p id="b5b2" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">You will notice that its challenging to choose a value of threshold that gives both good precision and recall and values. if the threshold is too high, you have a smaller number of true positive and a high number of false negatives. This decreases your recall; however, your precision score will be high. If you reduce the threshold too low, false positive will increase a lot, and precision will be less.</p><p id="1279" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Both precision and recall range from 0 to 1 and a value closer to 1 is better.</p><h1 id="868e" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">F1-Score:</h1><p id="157d" class="pw-post-body-paragraph kp kq jh kr b ks of ku kv kw og ky kz la oh lc ld le oi lg lh li oj lk ll lm ja fn" data-selectable-paragraph="">F1-score is a metric that combines both precision and recall. It is defined as a simple weighted average (harmonic mean) of precision and recall.</p><h1 id="645e" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">F1-Score = 2*precision x recall/(precision + recall)</h1><p id="8901" class="pw-post-body-paragraph kp kq jh kr b ks of ku kv kw og ky kz la oh lc ld le oi lg lh li oj lk ll lm ja fn" data-selectable-paragraph="">We can also find f1-score based on tp/fp and fn using below equation</p><h1 id="e614" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">F1-Score = 2TP/(2TP+FP+FN)</h1><pre class="mz na nb nc ha om fe on"><span id="30ae" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph="">def f1(y_true, y_pred):<br>    '''<br>    Function to calculate f1 score<br>    :param y_true: list of true values.<br>    :param y_pred: list of predicted values.<br>    :return: f1 score<br>    '''<br>    <br>    p = precision(y_true, y_pred)<br>    r = recall(y_true, y_pred)<br>    f1 = 2*p*r/(p+r)<br>    <br>    <br>    return f1</span><span id="9c40" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">from sklearn.metrics import f1_score <br>f1(y_test, y_pred), f1_score(y_test, y_pred)</span><span id="0ac8" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">(0.9543568464730291, 0.9543568464730291)</span></pre><p id="1426" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">From above we can observe that both our custom f1-score and sklearn f1-score values are same.</p><p id="9065" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Instead of looking at precision and recall individually, you can also just look at f1-score.</p><ul class=""><li id="8f49" class="ln lo jh kr b ks kt kw kx la lp le lq li lr lm ls lt lu lv fn" data-selectable-paragraph="">Same as for precision, recall and accuracy, F1-score also ranges from 0 to 1. The perfect prediction model has F1 score of 1. <strong class="kr ji">When dealing with datasets that have skewed targets, we should look at F1 (or precision and recall) instead of accuracy</strong>.</li></ul><p id="f010" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Other crucial terms</p><h1 id="0282" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">TPR or True positive rate, which is the same as recall.</h1><h1 id="66da" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">TPR = TP/(TP + FN)</h1><h1 id="a5a6" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">TPR or Recall is also know as sensitivity.</h1><pre class="mz na nb nc ha om fe on"><span id="66a1" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph="">def tpr(y_true, y_pred):<br>    '''<br>    Function that return TPR <br>    :param y_true: list of true values.<br>    :param y_pred: list of predicited values.<br>    :return tpr value<br>    '''<br>    tp = true_positive(y_true, y_pred)<br>    fn = false_negative(y_true, y_pred)<br>    try:<br>        tpr = tp/(tp+fn)<br>    except:<br>        tpr = 0<br>        <br>    return tpr</span></pre><h1 id="5167" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">FPR or False positive rate, which is defined as</h1><h1 id="890c" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">FPR = FP/(TN + FP)</h1><pre class="mz na nb nc ha om fe on"><span id="96a9" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph="">def fpr(y_true, y_pred):<br>    '''<br>    Function to return fpr<br>    :param y_true: list of true values.<br>    :param y_pred: list of predictied values.<br>    :return: fpr value<br>    '''<br>    <br>    fp = false_positive(y_true, y_pred)<br>    tn = true_negative(y_true, y_pred)<br>    try:<br>        fpr = fp/(tn+fp)<br>    except:<br>        fpr = 0<br>    return fpr</span></pre><h1 id="b27d" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">1- FPR is known as specifity or ture negative rate or TNR</h1><p id="59d0" class="pw-post-body-paragraph kp kq jh kr b ks of ku kv kw og ky kz la oh lc ld le oi lg lh li oj lk ll lm ja fn" data-selectable-paragraph="">There are a lot of terms, but the most important ones out of these are only <strong class="kr ji">TPR</strong> and <strong class="kr ji">FPR</strong>.</p><h1 id="af7d" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">Area under the curve or (area under the ROC(receiver operating characteristic curve) AUC</h1><p id="3a90" class="pw-post-body-paragraph kp kq jh kr b ks of ku kv kw og ky kz la oh lc ld le oi lg lh li oj lk ll lm ja fn" data-selectable-paragraph="">Let’s assume that we have only 15 samples and their target values are binary:</p><p id="24ea" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Actual targets : [0, 0,0,0,1,0,1,0,0,1,0,1,0,0,1,0,1]</p><p id="1c94" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">we train a model like the random forest, and we can get the probability of when a sample is positive.</p><p id="ae21" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Predicted probabilities for 1: [0.1, 0.3, 0.2, 0.6, 0.8, 0.05, 0.9, 0.5, 0.3, 0.66, 0.3, 0.2, 0.85, 0.15, 0.99]</p><p id="5353" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">For a typical threshold of &gt;=0.5, we can evaluate all the above values of precision, recall/TPR, F1 and FPR. But we can do the same if we choose the value of the threshold to be 0.4 or 0.6. In fact, we can choose any value between 0 and 1 and calculate all the metrics described above.</p><h2 id="a3d7" class="oo ni jh bn nj pa pb pc nn pd pe pf nr la pg ph nv le pi pj nz li pk pl od pm fn" data-selectable-paragraph="">Lets calculate only two values, through: TPR and FPR.</h2><pre class="mz na nb nc ha om fe on"><span id="e1f8" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph="">tpr_list = []<br>fpr_list = []<br><br># actual target <br>y_true = [0, 0,0,0,1,0,1,0,0,1,0,1,0,0,1]<br><br># predicted probabilities of a sample being 1<br>y_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05, 0.9, 0.5, 0.3, 0.66, 0.3, 0.2, 0.85, 0.15, 0.99] <br><br><br># Hand made threshold<br>threshold = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.99, 1.0 ]<br><br># loop over all threshold<br>for thresh in threshold:<br>    <br>    # calculate predictions for a given threshold<br>    temp_pred = [1 if x &gt;= thresh else 0 for x in y_pred]<br>    <br>    # calculate trp<br>    temp_tpr = tpr(y_true, list(temp_pred))<br>    <br>    # calculate fpr<br>    temp_fpr = fpr(y_true, list(temp_pred))<br>    <br>    # append tpr and fpr to listss<br>    tpr_list.append(temp_tpr)<br>    fpr_list.append(temp_fpr)</span><span id="114c" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">df = pd.DataFrame({'Threshold':threshold, 'tpr':tpr_list, 'fpr':fpr_list})<br>df</span></pre><p id="fd42" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Threshold tpr fpr 0 0.00 1.0 1.0 1 0.10 1.0 0.9 2 0.20 1.0 0.7 3 0.30 0.8 0.6 4 0.40 0.8 0.3 5 0.50 0.8 0.3 6 0.60 0.8 0.2 7 0.70 0.6 0.1 8 0.80 0.6 0.1 9 0.85 0.4 0.1 10 0.90 0.4 0.0 11 0.99 0.2 0.0 12 1.00 0.0 0.0</p><p id="6bc4" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">If we plot the above table as TPR on the y-axis and FPR on the X-axis, we will get a curve as shown below</p><pre class="mz na nb nc ha om fe on"><span id="eed0" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph="">plt.figure(figsize=(7,7))<br>plt.fill_between(fpr_list, tpr_list, alpha=0.4)<br>plt.plot(fpr_list, tpr_list, lw=3)<br>plt.xlim(0, 1.0)<br>plt.ylim(0, 1.0)<br>plt.xlabel('FPR', fontsize=15)<br>plt.ylabel('TPR', fontsize=15)<br>plt.show()</span></pre><figure class="mz na nb nc ha ki go gp paragraph-image"><div class="go gp pv"><img alt="" class="cf kj kk" src="https://miro.medium.com/max/568/0*Auqq4J0vXitIaAHZ.png" loading="lazy" role="presentation" width="454" height="434"></div></figure><p id="86f0" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">This curve is also known as the <strong class="kr ji">Receiver operating characteristic (ROC)</strong>. And if we calculate the area under the ROC curve. we are calculating another metric which is used very often when you have a dataset which has skewed binary targets.</p><p id="55e2" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">This metrics is known as the <strong class="kr ji">Area under ROC curve or Area under the curve</strong> or just simply <strong class="kr ji">AUC</strong>. there are many ways to calculate the area under the roc curve.</p><pre class="mz na nb nc ha om fe on"><span id="6527" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph="">from sklearn.metrics import roc_auc_score<br>roc_auc_score(y_true, y_pred)</span><span id="c022" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">0.8300000000000001</span></pre><h1 id="6e79" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">AUC values range from 0 to 1</h1><ul class=""><li id="0472" class="ln lo jh kr b ks of kw og la pw le px li py lm ls lt lu lv fn" data-selectable-paragraph="">AUC = 1 implies you have a perfect model. Most of the time, it means that you made some mistake with validation and should revisited data processing and validation pipeline of yours. if you didn't make any mistakes, then congratulations, you have the best model one can have for teh dataset you built it on.</li><li id="7600" class="ln lo jh kr b ks lw kw lx la ly le lz li ma lm ls lt lu lv fn" data-selectable-paragraph="">AUC = 0 implies your model is very bad( or its very good!) try inverting the labels .</li><li id="73f8" class="ln lo jh kr b ks lw kw lx la ly le lz li ma lm ls lt lu lv fn" data-selectable-paragraph="">AUC = 0.5 implies that your predictions are random. so for any binary classification problem, if I predict all target as 0.5, I will get an AUC of 0.5</li></ul><p id="8d0a" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">After calculating probabilities and AUC, you would want to make predictions on the test set. Depending on the problem and use-case, you might want to either have probailites or actual classes. if you want to have probabilities, its effortless. You already have them. If you want to have classes, you need to select a threshold. In the case of binary classification, you can do something like the following.</p><p id="e044" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph=""><strong class="kr ji">Prediction = probability &gt;= threshold</strong></p><p id="6e83" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Which means, that prediction is a new list which contains only binary variables. An item in prediciton is 1 if the probability is greater than or equal to a given <strong class="kr ji">threshold</strong> else the value is 0.</p><p id="6885" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">We can use <strong class="kr ji">ROC</strong> curve to choose this threshold!. The ROC curve will tell you how the threshold impacts false positive rate and true positive rate and thus, false positive and true positives. you should choose the threshold that is best suited for your problem and datasets.</p><h1 id="0349" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">Log Loss:</h1><p id="6418" class="pw-post-body-paragraph kp kq jh kr b ks of ku kv kw og ky kz la oh lc ld le oi lg lh li oj lk ll lm ja fn" data-selectable-paragraph="">In case of a binary classification problem. we define log loss as:</p><h1 id="6b08" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">Log Loss = -1.0 x (target x log(prediction) + (1-target) x log(1-prediction) )</h1><p id="7ec5" class="pw-post-body-paragraph kp kq jh kr b ks of ku kv kw og ky kz la oh lc ld le oi lg lh li oj lk ll lm ja fn" data-selectable-paragraph="">where target is either 0 or 1 and prediction is a probability of a sample belonging to class 1.</p><p id="619a" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">For multiple samples in the dataset, the log-loss over all samples is a mere average of all individual log losses. ONe thing to remember is that log loss penalizes quite high for an incorrect or a far-off prediciton</p><p id="092d" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">i.e log loss punishes you for being very sure and very wrong.</p><figure class="mz na nb nc ha ki"><div class="m l dq"><div class="ok ol l"></div></div></figure><pre class="mz na nb nc ha om fe on"><span id="43cd" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph="">log_loss(y_true, y_proba)</span><span id="e648" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">0.49882711861432294</span><span id="6a2f" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">from sklearn.metrics import log_loss<br>log_loss(y_true, y_proba)</span><span id="c61b" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">0.49882711861432294</span></pre><p id="c02a" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Thus, our implementation is correct. Implementation of log loss is easy interpretation may seem a bit difficult. You might remember that log loss penalizes a lot more than other metrics</p><h1 id="51c3" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">Multi-Class classification:</h1><p id="22a7" class="pw-post-body-paragraph kp kq jh kr b ks of ku kv kw og ky kz la oh lc ld le oi lg lh li oj lk ll lm ja fn" data-selectable-paragraph="">There are three different ways to calculate this which might get confusing from time to time. Lets assume we are interested in precision first. we know that precision depends on true positives and false positives.</p><h1 id="b95b" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">- Macro averaged precision:</h1><p id="04e9" class="pw-post-body-paragraph kp kq jh kr b ks of ku kv kw og ky kz la oh lc ld le oi lg lh li oj lk ll lm ja fn" data-selectable-paragraph="">Calculate precision for all individually and then average them.</p><h1 id="6810" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">- Micro averaged precision:</h1><p id="1af7" class="pw-post-body-paragraph kp kq jh kr b ks of ku kv kw og ky kz la oh lc ld le oi lg lh li oj lk ll lm ja fn" data-selectable-paragraph="">Calculate class wise true positive and false positive and then use that to calculate overall precision.</p><h1 id="0c5d" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">- Weighted precision:</h1><p id="f25c" class="pw-post-body-paragraph kp kq jh kr b ks of ku kv kw og ky kz la oh lc ld le oi lg lh li oj lk ll lm ja fn" data-selectable-paragraph="">Same as macro but in this case, it is weighted average depending on the number of items in each class.</p><figure class="mz na nb nc ha ki"><div class="m l dq"><div class="ok ol l"></div></div></figure><p id="e5ec" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Lets compare our implementation with the sklean to know if we implemented it right</p><pre class="mz na nb nc ha om fe on"><span id="6a53" class="fn oo ni jh op b do oq or l os" data-selectable-paragraph="">from sklearn import metrics</span><span id="32ed" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">y_true = [0,1,2,0,1,2,0,2,2]<br>y_pred = [0,2,1,0,2,1,0,0,2]</span><span id="d390" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">macro_precision(y_true, y_pred), metrics.precision_score(y_true, y_pred, average='macro')</span><span id="68f8" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">(0.3611111111111111, 0.3611111111111111)</span><span id="7e92" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">micro_precision(y_true, y_pred), metrics.precision_score(y_true, y_pred, average='micro')</span><span id="6f68" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">(0.4444444444444444, 0.4444444444444444)</span><span id="39b8" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">weighted_prediction(y_true, y_pred), metrics.precision_score(y_true, y_pred, average='weighted')</span><span id="436d" class="fn oo ni jh op b do ot ou ov ow ox or l os" data-selectable-paragraph="">(0.39814814814814814, 0.39814814814814814)</span></pre><p id="5dd6" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Similarly, we can implemented the <strong class="kr ji">recall, f1-score, AUC and log-loss</strong> for multi class.</p><h1 id="64e6" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">Confusion matrix:</h1><p id="0839" class="pw-post-body-paragraph kp kq jh kr b ks of ku kv kw og ky kz la oh lc ld le oi lg lh li oj lk ll lm ja fn" data-selectable-paragraph="">A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model.</p><p id="2e7a" class="pw-post-body-paragraph kp kq jh kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ja fn" data-selectable-paragraph="">Confusion matrices are widely used because they give a better idea of a model’s performance than classification accuracy does. For more information please check below reference link</p><h1 id="ef8c" class="nh ni jh bn nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fn" data-selectable-paragraph="">Reference links:</h1><ol class=""><li id="3649" class="ln lo jh kr b ks of kw og la pw le px li py lm pz lt lu lv fn" data-selectable-paragraph=""><a class="au ko" href="https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5" rel="noopener">https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5</a></li><li id="d681" class="ln lo jh kr b ks lw kw lx la ly le lz li ma lm pz lt lu lv fn" data-selectable-paragraph=""><a class="au ko" href="https://medium.com/@fardinahsan146/accuracy-is-not-accurate-6eb321f2999c" rel="noopener">https://medium.com/@fardinahsan146/accuracy-is-not-accurate-6eb321f2999c</a></li><li id="6139" class="ln lo jh kr b ks lw kw lx la ly le lz li ma lm pz lt lu lv fn" data-selectable-paragraph=""><a class="au ko" href="https://medium.datadriveninvestor.com/9-types-of-performance-evaluation-for-classification-machine-learning-modeling-c6e73e97e528" rel="noopener ugc nofollow" target="_blank">https://medium.datadriveninvestor.com/9-types-of-performance-evaluation-for-classification-machine-learning-modeling-c6e73e97e528</a></li><li id="4278" class="ln lo jh kr b ks lw kw lx la ly le lz li ma lm pz lt lu lv fn" data-selectable-paragraph=""><a class="au ko" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics</a></li><li id="ca0d" class="ln lo jh kr b ks lw kw lx la ly le lz li ma lm pz lt lu lv fn" data-selectable-paragraph=""><a class="au ko" href="https://www.amazon.in/Approaching-Almost-Machine-Learning-Problem-ebook/dp/B089P13QHT" rel="noopener ugc nofollow" target="_blank">https://www.amazon.in/Approaching-Almost-Machine-Learning-Problem-ebook/dp/B089P13QHT</a></li></ol></div></div>